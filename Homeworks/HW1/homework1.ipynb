{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Attack Prediction\n",
    "I will analyze the Heart Attack data set from Kaggle. I will compare the performance of different models: Logistic Regression and Random Forest.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "data = pd.read_csv('heart.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data analysis\n",
    "Data consists of 13 features and 1 column of labels (output). Dimension of the data is: (303,14). The features are:\n",
    "\n",
    "Age : Age of the patient - categorical\n",
    "\n",
    "Sex : Sex of the patient - categorical\n",
    "\n",
    "exang: exercise induced angina (1 = yes; 0 = no) - categorical\n",
    "\n",
    "ca: number of major vessels (0-3) - categorical\n",
    "\n",
    "cp : Chest Pain type chest pain type - categorical\n",
    "\n",
    "    Value 1: typical angina\n",
    "    Value 2: atypical angina\n",
    "    Value 3: non-anginal pain\n",
    "    Value 4: asymptomatic\n",
    "\n",
    "trtbps : resting blood pressure (in mm Hg) - continuous\n",
    "\n",
    "chol : cholestoral in mg/dl fetched via BMI sensor - continuous\n",
    "\n",
    "fbs : (fasting blood sugar > 120 mg/dl) (1 = true; 0 = false) - categorical\n",
    "\n",
    "rest_ecg : resting electrocardiographic results - categorical\n",
    "\n",
    "    Value 0: normal\n",
    "    Value 1: having ST-T wave abnormality (T wave inversions and/or ST elevation or depression of > 0.05 mV)\n",
    "    Value 2: showing probable or definite left ventricular hypertrophy by Estes' criteria\n",
    "\n",
    "thalach : maximum heart rate achieved - continuous\n",
    "\n",
    "\n",
    "\n",
    "output : 0 = less chance of heart attack 1= more chance of heart attack - categorical\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  sex  cp  trtbps  chol  fbs  restecg  thalachh  exng  oldpeak  slp  \\\n",
      "0   63    1   3     145   233    1        0       150     0      2.3    0   \n",
      "1   37    1   2     130   250    0        1       187     0      3.5    0   \n",
      "2   41    0   1     130   204    0        0       172     0      1.4    2   \n",
      "3   56    1   1     120   236    0        1       178     0      0.8    2   \n",
      "4   57    0   0     120   354    0        1       163     1      0.6    2   \n",
      "\n",
      "   caa  thall  output  \n",
      "0    0      1       1  \n",
      "1    0      2       1  \n",
      "2    0      2       1  \n",
      "3    0      2       1  \n",
      "4    0      2       1  \n",
      "(303, 14)\n"
     ]
    }
   ],
   "source": [
    "# Analysing data\n",
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data does not have any missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Columns age, trtbps, chol, thalachh, oldpeak are numerical columns. The rest are categorical columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "age         0\n",
      "sex         0\n",
      "cp          0\n",
      "trtbps      0\n",
      "chol        0\n",
      "fbs         0\n",
      "restecg     0\n",
      "thalachh    0\n",
      "exng        0\n",
      "oldpeak     0\n",
      "slp         0\n",
      "caa         0\n",
      "thall       0\n",
      "output      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Missing values check\n",
    "print(data.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before creating a model, I will `normalize` and split the data to train and test sets. I will use 80% of the data for training and 20% for testing.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['age'] = MinMaxScaler().fit_transform(data['age'].values.reshape(-1,1)) # Normalizing age column\n",
    "data['trtbps'] = MinMaxScaler().fit_transform(data['trtbps'].values.reshape(-1,1)) # Normalizing trtbps column\n",
    "data['chol'] = MinMaxScaler().fit_transform(data['chol'].values.reshape(-1,1)) # Normalizing chol column\n",
    "data['thalachh'] = MinMaxScaler().fit_transform(data['thalachh'].values.reshape(-1,1)) # Normalizing thalachh column\n",
    "data['oldpeak'] = MinMaxScaler().fit_transform(data['oldpeak'].values.reshape(-1,1)) # Normalizing oldpeak column\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>sex</th>\n",
       "      <th>cp</th>\n",
       "      <th>trtbps</th>\n",
       "      <th>chol</th>\n",
       "      <th>fbs</th>\n",
       "      <th>restecg</th>\n",
       "      <th>thalachh</th>\n",
       "      <th>exng</th>\n",
       "      <th>oldpeak</th>\n",
       "      <th>slp</th>\n",
       "      <th>caa</th>\n",
       "      <th>thall</th>\n",
       "      <th>output</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.708333</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.481132</td>\n",
       "      <td>0.244292</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.603053</td>\n",
       "      <td>0</td>\n",
       "      <td>0.370968</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.166667</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.283105</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.885496</td>\n",
       "      <td>0</td>\n",
       "      <td>0.564516</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.250000</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.770992</td>\n",
       "      <td>0</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.562500</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.251142</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.816794</td>\n",
       "      <td>0</td>\n",
       "      <td>0.129032</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.245283</td>\n",
       "      <td>0.520548</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.702290</td>\n",
       "      <td>1</td>\n",
       "      <td>0.096774</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>298</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.433962</td>\n",
       "      <td>0.262557</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.396947</td>\n",
       "      <td>1</td>\n",
       "      <td>0.032258</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>299</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0.150943</td>\n",
       "      <td>0.315068</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.465649</td>\n",
       "      <td>0</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>300</th>\n",
       "      <td>0.812500</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.471698</td>\n",
       "      <td>0.152968</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.534351</td>\n",
       "      <td>0</td>\n",
       "      <td>0.548387</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>301</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.011416</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.335878</td>\n",
       "      <td>1</td>\n",
       "      <td>0.193548</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>302</th>\n",
       "      <td>0.583333</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.339623</td>\n",
       "      <td>0.251142</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.786260</td>\n",
       "      <td>0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>303 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          age  sex  cp    trtbps      chol  fbs  restecg  thalachh  exng  \\\n",
       "0    0.708333    1   3  0.481132  0.244292    1        0  0.603053     0   \n",
       "1    0.166667    1   2  0.339623  0.283105    0        1  0.885496     0   \n",
       "2    0.250000    0   1  0.339623  0.178082    0        0  0.770992     0   \n",
       "3    0.562500    1   1  0.245283  0.251142    0        1  0.816794     0   \n",
       "4    0.583333    0   0  0.245283  0.520548    0        1  0.702290     1   \n",
       "..        ...  ...  ..       ...       ...  ...      ...       ...   ...   \n",
       "298  0.583333    0   0  0.433962  0.262557    0        1  0.396947     1   \n",
       "299  0.333333    1   3  0.150943  0.315068    0        1  0.465649     0   \n",
       "300  0.812500    1   0  0.471698  0.152968    1        1  0.534351     0   \n",
       "301  0.583333    1   0  0.339623  0.011416    0        1  0.335878     1   \n",
       "302  0.583333    0   1  0.339623  0.251142    0        0  0.786260     0   \n",
       "\n",
       "      oldpeak  slp  caa  thall  output  \n",
       "0    0.370968    0    0      1       1  \n",
       "1    0.564516    0    0      2       1  \n",
       "2    0.225806    2    0      2       1  \n",
       "3    0.129032    2    0      2       1  \n",
       "4    0.096774    2    0      2       1  \n",
       "..        ...  ...  ...    ...     ...  \n",
       "298  0.032258    1    0      3       0  \n",
       "299  0.193548    1    0      3       0  \n",
       "300  0.548387    1    2      3       0  \n",
       "301  0.193548    1    1      3       0  \n",
       "302  0.000000    1    1      2       0  \n",
       "\n",
       "[303 rows x 14 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = data['output'] # Target variable\n",
    "X = data.drop(['output'], axis=1) # Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train test split\n",
    "\n",
    "I will use `train_test_split` from `sklearn.model_selection` to split the data to train and test sets. I will use 80% of the data for training and 20% for testing. I will use `random_state=42` to make the results reproducible."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train test split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(242, 13)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression\n",
    "\n",
    "The first model I will use is `LogisticRegression` from `sklearn.linear_model`. I will use default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "log_reg = LogisticRegression() # Creating model with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg.fit(X_train, y_train) # Fitting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Performance\n",
    "Our logistic regression model achieved an accuracy of ~84% on the test set. The model is not overfitting, because the accuracy on the test and training set are similar. We can interpret achieved accuracy as follows: for 100 patients on average 16 of them would get the wrong diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy on test set: 0.8360655737704918\n",
      "Average accuracy on train set: 0.8553719008264463\n"
     ]
    }
   ],
   "source": [
    "print(f'Average accuracy on test set: {log_reg.score(X_test, y_test)}') # Accuracy on test set\n",
    "print(f'Average accuracy on train set: {log_reg.score(X_train, y_train)}') # Accuracy on train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The precision of the model is ~0.86. This means that for 100 patients that the model predicted to have heart disease, 86 of them actually have heart disease.\n",
    "\n",
    "The sensitivity of the model is ~0.81. This means that for 100 patients that actually have heart disease, 81 of them are predicted to have heart disease by the model.\n",
    "\n",
    "The F1 score of the model is ~0.83. This means that the model is good at predicting heart disease. The F1 score is the harmonic mean of precision and sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8666666666666667\n",
      "Sensitivity: 0.8125\n",
      "F1 score: 0.8387096774193549\n"
     ]
    }
   ],
   "source": [
    "# Model performance\n",
    "print(f'Precision: {sklearn.metrics.precision_score(y_test, log_reg.predict(X_test))}')\n",
    "print(f'Sensitivity: {sklearn.metrics.recall_score(y_test, log_reg.predict(X_test))}')\n",
    "print(f'F1 score: {sklearn.metrics.f1_score(y_test, log_reg.predict(X_test))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make the coefficients easy to interpret we need to exponentiate them. The exponentiated coefficients are as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.60125786, 0.27105444, 2.09489142, 0.45127838, 0.65790576,\n",
       "        1.20779825, 1.54097501, 2.55902379, 0.32690452, 0.20597206,\n",
       "        2.51901646, 0.45119277, 0.42401766]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.exp(log_reg.coef_) # Coefficients of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coefficients interpretation\n",
    "The general interpretation of the coefficients is as follows: For every `one-unit` increase in the variable, the odds that the patient has `more` chance of heart attack are the `coefficient times` as large as the odds that the patient has a lower chance of heart attack when all other variables are `held constant.\n",
    "\n",
    "Values of the `coefficients` are all `similar` and in `reasonable range`, suggesting that all features are `important` for the model, therefore we can use `all of them.\n",
    "\n",
    "age ~ 0.6\n",
    "\n",
    "For every one-unit increase in age, the odds that the patient has more chance of heart attack are 0.6 times as large as the odds that the patient has a lower chance of heart attack when all other variables are held constant. This means that the `older` the patient is, the `more chance` of a heart attack he has.\n",
    "\n",
    "sex ~ 0.27 \n",
    "\n",
    "The odds of Males having more chance of heart attack are 0.27 times as large as the odds of Female ceteris paribus.\n",
    "\n",
    "cp ~ 2.1\n",
    "\n",
    "As variable CP increases by one unit, the odds that the patient has more chance of heart attack are 2.1 times as large as the odds that the patient has a lower chance of heart attack when all other variables are held constant. \n",
    "\n",
    "trtbps ~ 0.45\n",
    "\n",
    "For every one-unit increase in trtbps, the odds that the patient has more chance of heart attack are 0.45 times as large as the odds that the patient has a lower chance of heart attack when all other variables are held constant.\n",
    "\n",
    "chol ~ 0.66\n",
    "\n",
    "For every one-unit increase in chol, the odds that the patient has more chance of heart attack are 0.66 times as large as the odds that the patient has a lower chance of heart attack when all other variables are held constant.\n",
    "\n",
    "fbs ~ 1.2\n",
    "\n",
    "The odds of having more chance of heart attack for patients with fbs > 120 mg/dl are 1.2 times as large as the odds of having a lower chance of heart attack ceteris paribus.\n",
    "\n",
    "restecg ~ 1.54\n",
    "\n",
    "The odds of having more chance of heart attack for patients with restecg = 1 are 1.54 times as large as the odds of having a lower chance of heart attack ceteris paribus.\n",
    "\n",
    "thalachh ~ 2.56\n",
    "\n",
    "For every one-unit increase in thalachh, the odds that the patient has more chance of heart attack are 2.56 times as large as the odds that the patient has a lower chance of heart attack when all other variables are held constant.\n",
    "\n",
    "exng ~ 0.32\n",
    "\n",
    "The odds of having more chance of heart attack for patients with exercise-induced angina are 0.32 times as large as the odds of patients without induced angina ceteris paribus.\n",
    "\n",
    "oldpeak ~ 0.20\n",
    "\n",
    "For every one-unit increase in oldpeak, the odds that the patient has more chance of heart attack are 0.20 times as large as the odds the patient has a lower chance of heart attack when all other variables are held constant.\n",
    "\n",
    "etc.\n",
    "\n",
    "\n",
    "`Comment: I used the word 'unit' because the data is normalized.`\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest\n",
    "The next model I will use is `RandomForestClassifier` from `sklearn.ensemble`. I will use default parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RandomForestClassifier\n",
    "forest = RandomForestClassifier() # Creating model with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.fit(X_train, y_train) # Fitting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a high chance that the model is `overfitting` because the accuracy on the training set is perfect, while the accuracy on test set is ~0.85. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy on test set: 0.8524590163934426\n",
      "Average accuracy on train set: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Average accuracy on test set: {forest.score(X_test, y_test)}') # Accuracy on test set\n",
    "print(f'Average accuracy on train set: {forest.score(X_train, y_train)}') # Accuracy on train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can interpret achieved `accuracy` (on test set) as follows: for 100 patients on average 15 of them would get wrong diagnosis.\n",
    "\n",
    "`Precision` of the model is ~0.87. This means that for 100 patients that the model predicted to have heart disease, 87 of them actually have heart disease.\n",
    "\n",
    "`Sensitivity` of the model is ~0.84. This means that for 100 patients that actually have heart disease, 84 of them are predicted to have heart disease by the model.\n",
    "\n",
    "`F1 score` of the model is ~0.86. This means that the model is good at predicting heart disease. The F1 score is the harmonic mean of precision and sensitivity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8709677419354839\n",
      "Sensitivity: 0.84375\n",
      "F1 score: 0.8571428571428571\n"
     ]
    }
   ],
   "source": [
    "print(f'Precision: {sklearn.metrics.precision_score(y_test, forest.predict(X_test))}')\n",
    "print(f'Sensitivity: {sklearn.metrics.recall_score(y_test, forest.predict(X_test))}')\n",
    "print(f'F1 score: {sklearn.metrics.f1_score(y_test, forest.predict(X_test))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking feature importance\n",
    "\n",
    "The score for feature importance is the `mean decrease in impurity` (MDI) across all trees in the forest. The higher the score, the more important the feature. The score for each feature is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09379158, 0.04658748, 0.11263448, 0.06518745, 0.08287688,\n",
       "       0.01207317, 0.01609314, 0.10488445, 0.08243148, 0.13642377,\n",
       "       0.04464606, 0.12251505, 0.07985502])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.feature_importances_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The sixth score corresponding to feature `fbs` is the lowest, which means that the feature is the least important. The rest of the features are important. I will create a new model with only important features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.drop('fbs', axis = 1) # Dropping fbs column\n",
    "X_test = X_test.drop('fbs', axis = 1) # Dropping fbs column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier()"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "forest.fit(X_train, y_train) # Fitting model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interestingly, model with only important features achieved lower `accuracy` of ~0.84 on test set compared to the one with all features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average accuracy on test set: 0.8524590163934426\n",
      "Average accuracy on train set: 1.0\n"
     ]
    }
   ],
   "source": [
    "print(f'Average accuracy on test set: {forest.score(X_test, y_test)}') # Accuracy on test set\n",
    "print(f'Average accuracy on train set: {forest.score(X_train, y_train)}') # Accuracy on train set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of the metrics are also lower."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.8484848484848485\n",
      "Sensitivity: 0.875\n",
      "F1 score: 0.8615384615384615\n"
     ]
    }
   ],
   "source": [
    "print(f'Precision: {sklearn.metrics.precision_score(y_test, forest.predict(X_test))}')\n",
    "print(f'Sensitivity: {sklearn.metrics.recall_score(y_test, forest.predict(X_test))}')\n",
    "print(f'F1 score: {sklearn.metrics.f1_score(y_test, forest.predict(X_test))}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "In this notebook, I analyzed the Heart Attack dataset. I created a logistic regression model and a random forest model. I used default parameters for both models. I used accuracy, precision, sensitivity, and F1 score to evaluate the models. Based on the values of coefficients of logistic regression I concluded that all of the features are important. I also checked the feature importance for the random forest model. I created a new model with only important features. The model with all features achieved better results than the model with only important features. Overall, the random forest model achieved better results than the logistic regression model. It is important to note that though the logistic regression model achieved worse results, it is easier to interpret the coefficients of the logistic regression model and therefore it is easier to explain the model to the non-technical audience."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.12 ('elegans': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "64b1c5580a8467aa2467f3fc1381ee855b24362c342946b6341fb6fd718a70c0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
