<!DOCTYPE html>
<html lang="en"><head>
<script src="03_lime_files/libs/clipboard/clipboard.min.js"></script>
<script src="03_lime_files/libs/quarto-html/tabby.min.js"></script>
<script src="03_lime_files/libs/quarto-html/popper.min.js"></script>
<script src="03_lime_files/libs/quarto-html/tippy.umd.min.js"></script>
<link href="03_lime_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="03_lime_files/libs/quarto-html/quarto-html.min.css" rel="stylesheet" data-mode="light">
<link href="03_lime_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles"><meta charset="utf-8">
  <meta name="generator" content="quarto-1.0.36">

  <meta name="author" content="Przemysław Biecek">
  <title>LIME and friends</title>
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no, minimal-ui">
  <link rel="stylesheet" href="03_lime_files/libs/revealjs/dist/reset.css">
  <link rel="stylesheet" href="03_lime_files/libs/revealjs/dist/reveal.css">
  <style>
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <link rel="stylesheet" href="03_lime_files/libs/revealjs/dist/theme/quarto.css" id="theme">
  <link href="03_lime_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.css" rel="stylesheet">
  <link href="03_lime_files/libs/revealjs/plugin/reveal-menu/menu.css" rel="stylesheet">
  <link href="03_lime_files/libs/revealjs/plugin/reveal-menu/quarto-menu.css" rel="stylesheet">
  <link href="03_lime_files/libs/revealjs/plugin/reveal-chalkboard/font-awesome/css/all.css" rel="stylesheet">
  <link href="03_lime_files/libs/revealjs/plugin/reveal-chalkboard/style.css" rel="stylesheet">
  <link href="03_lime_files/libs/revealjs/plugin/quarto-support/footer.css" rel="stylesheet">
  <style type="text/css">

  .callout {
    margin-top: 1em;
    margin-bottom: 1em;  
    border-radius: .25rem;
  }

  .callout.callout-style-simple { 
    padding: 0em 0.5em;
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
    display: flex;
  }

  .callout.callout-style-default {
    border-left: solid #acacac .3rem;
    border-right: solid 1px silver;
    border-top: solid 1px silver;
    border-bottom: solid 1px silver;
  }

  .callout .callout-body-container {
    flex-grow: 1;
  }

  .callout.callout-style-simple .callout-body {
    font-size: 1rem;
    font-weight: 400;
  }

  .callout.callout-style-default .callout-body {
    font-size: 0.9rem;
    font-weight: 400;
  }

  .callout.callout-captioned.callout-style-simple .callout-body {
    margin-top: 0.2em;
  }

  .callout:not(.callout-captioned) .callout-body {
      display: flex;
  }

  .callout:not(.no-icon).callout-captioned.callout-style-simple .callout-content {
    padding-left: 1.6em;
  }

  .callout.callout-captioned .callout-header {
    padding-top: 0.2em;
    margin-bottom: -0.2em;
  }

  .callout.callout-captioned .callout-caption  p {
    margin-top: 0.5em;
    margin-bottom: 0.5em;
  }
    
  .callout.callout-captioned.callout-style-simple .callout-content  p {
    margin-top: 0;
  }

  .callout.callout-captioned.callout-style-default .callout-content  p {
    margin-top: 0.7em;
  }

  .callout.callout-style-simple div.callout-caption {
    border-bottom: none;
    font-size: .9rem;
    font-weight: 600;
    opacity: 75%;
  }

  .callout.callout-style-default  div.callout-caption {
    border-bottom: none;
    font-weight: 600;
    opacity: 85%;
    font-size: 0.9rem;
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-default div.callout-content {
    padding-left: 0.5em;
    padding-right: 0.5em;
  }

  .callout.callout-style-simple .callout-icon::before {
    height: 1rem;
    width: 1rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 1rem 1rem;
  }

  .callout.callout-style-default .callout-icon::before {
    height: 0.9rem;
    width: 0.9rem;
    display: inline-block;
    content: "";
    background-repeat: no-repeat;
    background-size: 0.9rem 0.9rem;
  }

  .callout-caption {
    display: flex
  }
    
  .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  .callout.no-icon::before {
    display: none !important;
  }

  .callout.callout-captioned .callout-body > .callout-content > :last-child {
    margin-bottom: 0.5rem;
  }

  .callout.callout-captioned .callout-icon::before {
    margin-top: .5rem;
    padding-right: .5rem;
  }

  .callout:not(.callout-captioned) .callout-icon::before {
    margin-top: 1rem;
    padding-right: .5rem;
  }

  /* Callout Types */

  div.callout-note {
    border-left-color: #4582ec !important;
  }

  div.callout-note .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEU0lEQVRYCcVXTWhcVRQ+586kSUMMxkyaElstCto2SIhitS5Ek8xUKV2poatCcVHtUlFQk8mbaaziwpWgglJwVaquitBOfhQXFlqlzSJpFSpIYyXNjBNiTCck7x2/8/LeNDOZxDuEkgOXe++553zfefee+/OYLOXFk3+1LLrRdiO81yNqZ6K9cG0P3MeFaMIQjXssE8Z1JzLO9ls20MBZX7oG8w9GxB0goaPrW5aNMp1yOZIa7Wv6o2ykpLtmAPs/vrG14Z+6d4jpbSKuhdcSyq9wGMPXjonwmESXrriLzFGOdDBLB8Y6MNYBu0dRokSygMA/mrun8MGFN3behm6VVAwg4WR3i6FvYK1T7MHo9BK7ydH+1uurECoouk5MPRyVSBrBHMYwVobG2aOXM07sWrn5qgB60rc6mcwIDJtQrnrEr44kmy+UO9r0u9O5/YbkS9juQckLed3DyW2XV/qWBBB3ptvI8EUY3I9p/67OW+g967TNr3Sotn3IuVlfMLVnsBwH4fsnebJvyGm5GeIUA3jljERmrv49SizPYuq+z7c2H/jlGC+Ghhupn/hcapqmcudB9jwJ/3jvnvu6vu5lVzF1fXyZuZZ7U8nRmVzytvT+H3kilYvH09mLWrQdwFSsFEsxFVs5fK7A0g8gMZjbif4ACpKbjv7gNGaD8bUrlk8x+KRflttr22JEMRUbTUwwDQScyzPgedQHZT0xnx7ujw2jfVfExwYHwOsDTjLdJ2ebmeQIlJ7neo41s/DrsL3kl+W2lWvAga0tR3zueGr6GL78M3ifH0rGXrBC2aAR8uYcIA5gwV8zIE8onoh8u0Fca/ciF7j1uOzEnqcIm59sEXoGc0+z6+H45V1CvAvHcD7THztu669cnp+L0okAeIc6zjbM/24LgGM1gZk7jnRu1aQWoU9sfUOuhrmtaPIO3YY1KLLWZaEO5TKUbMY5zx8W9UJ6elpLwKXbsaZ4EFl7B4bMtDv0iRipKoDQT2sNQI9b1utXFdYisi+wzZ/ri/1m7QfDgEuvgUUEIJPq3DhX/5DWNqIXDOweC2wvIR90Oq3lDpdMIgD2r0dXvGdsEW5H6x6HLRJYU7C69VefO1x8Gde1ZFSJLfWS1jbCnhtOPxmpfv2LXOA2Xk2tvnwKKPFuZ/oRmwBwqRQDcKNeVQkYcOjtWVBuM/JuYw5b6isojIkYxyYAFn5K7ZBF10fea52y8QltAg6jnMqNHFBmGkQ1j+U43HMi2xMar1Nv0zGsf1s8nUsmUtPOOrbFIR8bHFDMB5zL13Gmr/kGlCkUzedTzzmzsaJXhYawnA3UmARpiYj5ooJZiUoxFRtK3X6pgNPv+IZVPcnwbOl6f+aBaO1CNvPW9n9LmCp01nuSaTRF2YxHqZ8DYQT6WsXT+RD6eUztwYLZ8rM+rcPxamv1VQzFUkzFXvkiVrySGQgJNvXHJAxiU3/NwiC03rSf05VBaPtu/Z7/B8Yn/w7eguloAAAAAElFTkSuQmCC');
  }

  div.callout-note.callout-style-default .callout-caption {
    background-color: #dae6fb
  }

  div.callout-important {
    border-left-color: #d9534f !important;
  }

  div.callout-important .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAEKklEQVRYCcVXTWhcVRS+575MJym48A+hSRFr00ySRQhURRfd2HYjk2SSTokuBCkU2o0LoSKKraKIBTcuFCoidGFD08nkBzdREbpQ1EDNIv8qSGMFUboImMSZd4/f9zJv8ibJMC8xJQfO3HPPPef7zrvvvnvviIkpC9nsw0UttFunbUhpFzFtarSd6WJkStVMw5xyVqYTvkwfzuf/5FgtkVoB0729j1rjXwThS7Vio+Mo6DNnvLfahoZ+i/o32lULuJ3NNiz7q6+pyAUkJaFF6JwaM2lUJlV0MlnQn5aTRbEu0SEqHUa0A4AdiGuB1kFXRfVyg5d87+Dg4DL6m2TLAub60ilj7A1Ec4odSAc8X95sHh7+ZRPCFo6Fnp7HfU/fBng/hi10CjCnWnJjsxvDNxWw0NfV6Rv5GgP3I3jGWXumdTD/3cbEOP2ZbOZp69yniG3FQ9z1jD7bnBu9Fc2tKGC2q+uAJOQHBDRiZX1x36o7fWBs7J9ownbtO+n0/qWkvW7UPIfc37WgT6ZGR++EOJyeQDSb9UB+DZ1G6DdLDzyS+b/kBCYGsYgJbSQHuThGKRcw5xdeQf8YdNHsc6ePXrlSYMBuSIAFTGAtQo+VuALo4BX83N190NWZWbynBjhOHsmNfFWLeL6v+ynsA58zDvvAC8j5PkbOcXCMg2PZFk3q8MjI7WAG/Dp9AwP7jdGBOOQkAvlFUB+irtm16I1Zw9YBcpGTGXYmk3kQIC/Cds55l+iMI3jqhjAuaoe+am2Jw5GT3Nbz3CkE12NavmzN5+erJW7046n/CH1RO/RVa8lBLozXk9uqykkGAyRXLWlLv5jyp4RFsG5vGVzpDLnIjTWgnRy2Rr+tDKvRc7Y8AyZq10jj8DqXdnIRNtFZb+t/ZRtXcDiVnzpqx8mPcDWxgARUqx0W1QB9MeUZiNrV4qP+Ehc+BpNgATsTX8ozYKL2NtFYAHc84fG7ndxUPr+AR/iQSns7uSUufAymwDOb2+NjK27lEFocm/EE2WpyIy/Hi66MWuMKJn8RvxIcj87IM5Vh9663ziW36kR0HNenXuxmfaD8JC7tfKbrhFr7LiZCrMjrzTeGx+PmkosrkNzW94ObzwocJ7A1HokLolY+AvkTiD/q1H0cN48c5EL8Crkttsa/AXQVDmutfyku0E7jShx49XqV3MFK8IryDhYVbj7Sj2P2eBxwcXoe8T8idsKKPRcnZw1b+slFTubwUwhktrfnAt7J++jwQtLZcm3sr9LQrjRzz6cfMv9aLvgmnAGvpoaGLxM4mAEaLV7iAzQ3oU0IvD5x9ix3yF2RAAuYAOO2f7PEFWCXZ4C9Pb2UsgDeVnFSpbFK7/IWu7TPTvBqzbGdCHOJQSxiEjt6IyZmxQyEJHv6xyQsYk//moVFsN2zP6fRImjfq7/n/wFDguUQFNEwugAAAABJRU5ErkJggg==');
  }

  div.callout-important.callout-style-default .callout-caption {
    background-color: #f7dddc
  }

  div.callout-warning {
    border-left-color: #f0ad4e !important;
  }

  div.callout-warning .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAAETklEQVRYCeVWW2gcVRg+58yaTUnizqbipZeX4uWhBEniBaoUX1Ioze52t7sRq6APio9V9MEaoWlVsFasRq0gltaAPuxms8lu0gcviE/FFOstVbSIxgcv6SU7EZqmdc7v9+9mJtNks51NTUH84ed889/PP+cmxP+d5FIbMJmNbpREu4WUkiTtCicKny0l1pIKmBzovF2S+hIJHX8iEu3hZJ5lNZGqyRrGSIQpq15AzF28jgpeY6yk6GVdrfFqdrD6Iw+QlB8g0YS2g7dyQmXM/IDhBhT0UCiRf59lfqmmDvzRt6kByV/m4JjtzuaujMUM2c5Z2d6JdKrRb3K2q6mA+oYVz8JnDdKPmmNthzkAk/lN63sYPgevrguc72aZX/L9C6x09GYyxBgCX4NlvyGUHOKELlm5rXeR1kchuChJt4SSwyddZRXgvwMGvYo4QSlk3/zkHD8UHxwVJA6zjZZqP8v8kK8OWLnIZtLyCAJagYC4rTGW/9Pqj92N/c+LUaAj27movwbi19tk/whRCIE7Q9vyI6yvRpftAKVTdUjOW40X3h5OXsKCdmFcx0xlLJoSuQngnrJe7Kcjm4OMq9FlC7CMmScQANuNvjfP3PjGXDBaUQmbp296S5L4DrpbrHN1T87ZVEZVCzg1FF0Ft+dKrlLukI+/c9ENo+TvlTDbYFvuKPtQ9+l052rXrgKoWkDAFnvh0wTOmYn8R5f4k/jN/fZiCM1tQx9jQQ4ANhqG4hiL0qIFTGViG9DKB7GYzgubnpofgYRwO+DFjh0Zin2m4b/97EDkXkc+f6xYAPX0KK2I/7fUQuwzuwo/L3AkcjugPNixC8cHf0FyPjWlItmLxWw4Ou9YsQCr5fijMGoD/zpdRy95HRysyXA74MWOnscpO4j2y3HAVisw85hX5+AFBRSHt4ShfLFkIMXTqyKFc46xdzQM6XbAi702a7sy04J0+feReMFKp5q9esYLCqAZYw/k14E/xcLLsFElaornTuJB0svMuJINy8xkIYuL+xPAlWRceH6+HX7THJ0djLUom46zREu7tTkxwmf/FdOZ/sh6Q8qvEAiHpm4PJ4a/doJe0gH1t+aHRgCzOvBvJedEK5OFE5jpm4AGP2a8Dxe3gGJ/pAutug9Gp6he92CsSsWBaEcxGx0FHytmIpuqGkOpldqNYQK8cSoXvd+xLxXADw0kf6UkJNFtdo5MOgaLjiQOQHcn+A6h5NuL2s0qsC2LOM75PcF3yr5STuBSAcGG+meA14K/CI21HcS4LBT6tv0QAh8Dr5l93AhZzG5ZJ4VxAqdZUEl9z7WJ4aN+svMvwHHL21UKTd1mqvChH7/Za5xzXBBKrUcB0TQ+Ulgkfbi/H/YT5EptrGzsEK7tR1B7ln9BBwckYfMiuSqklSznIuoIIOM42MQO+QnduCoFCI0bpkzjCjddHPN/F+2Yu+sd9bKNpVwHhbS3LluK/0zgfwD0xYI5dXuzlQAAAABJRU5ErkJggg==');
  }

  div.callout-warning.callout-style-default .callout-caption {
    background-color: #fcefdc
  }

  div.callout-tip {
    border-left-color: #02b875 !important;
  }

  div.callout-tip .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAADr0lEQVRYCe1XTWgTQRj9ZjZV8a9SPIkKgj8I1bMHsUWrqYLVg4Ue6v9BwZOxSYsIerFao7UiUryIqJcqgtpimhbBXoSCVxUFe9CTiogUrUp2Pt+3aUI2u5vdNh4dmMzOzHvvezuz8xNFM0mjnbXaNu1MvFWRXkXEyE6aYOYJpdW4IXuA4r0fo8qqSMDBU0v1HJUgVieAXxzCsdE/YJTdFcVIZQNMyhruOMJKXYFoLfIfIvVIMWdsrd+Rpd86ZmyzzjJmLStqRn0v8lzkb4rVIXvnpScOJuAn2ACC65FkPzEdEy4TPWRLJ2h7z4cArXzzaOdKlbOvKKX25Wl00jSnrwVxAg3o4dRxhO13RBSdNvH0xSARv3adTXbBdTf64IWO2vH0LT+cv4GR1DJt+DUItaQogeBX/chhbTBxEiZ6gftlDNXTrvT7co4ub5A6gp9HIcHvzTa46OS5fBeP87Qm0fQkr4FsYgVQ7Qg+ZayaDg9jhg1GkWj8RG6lkeSacrrHgDaxdoBiZPg+NXV/KifMuB6//JmYH4CntVEHy/keA6x4h4CU5oFy8GzrBS18cLJMXcljAKB6INjWsRcuZBWVaS3GDrqB7rdapVIeA+isQ57Eev9eCqzqOa81CY05VLd6SamW2wA2H3SiTbnbSxmzfp7WtKZkqy4mdyAlGx7ennghYf8voqp9cLSgKdqNfa6RdRsAAkPwRuJZNbpByn+RrJi1RXTwdi8RQF6ymDwGMAtZ6TVE+4uoKh+MYkcLsT0Hk8eAienbiGdjJHZTpmNjlbFJNKDVAp2fJlYju6IreQxQ08UJDNYdoLSl6AadO+fFuCQqVMB1NJwPm69T04Wv5WhfcWyfXQB+wXRs1pt+nCknRa0LVzSA/2B+a9+zQJadb7IyyV24YAxKp2Jqs3emZTuNnKxsah+uabKbMk7CbTgJx/zIgQYErIeTKRQ9yD9wxVof5YolPHqaWo7TD6tJlh7jQnK5z2n3+fGdggIOx2kaa2YI9QWarc5Ce1ipNWMKeSG4DysFF52KBmTNMmn5HqCFkwy34rDg05gDwgH3bBi+sgFhN/e8QvRn8kbamCOhgrZ9GJhFDgfcMHzFb6BAtjKpFhzTjwv1KCVuxHvCbsSiEz4CANnj84cwHdFXAbAOJ4LTSAawGWFn5tDhLMYz6nWeU2wJfIhmIJBefcd/A5FWQWGgrWzyORZ3Q6HuV+Jf0Bj+BTX69fm1zWgK7By1YTXchFDORywnfQ7GpzOo6S+qECrsx2ifVQAAAABJRU5ErkJggg==');
  }

  div.callout-tip.callout-style-default .callout-caption {
    background-color: #ccf1e3
  }

  div.callout-caution {
    border-left-color: #fd7e14 !important;
  }

  div.callout-caution .callout-icon::before {
    background-image: url('data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAYAAABzenr0AAAAAXNSR0IArs4c6QAAAERlWElmTU0AKgAAAAgAAYdpAAQAAAABAAAAGgAAAAAAA6ABAAMAAAABAAEAAKACAAQAAAABAAAAIKADAAQAAAABAAAAIAAAAACshmLzAAACV0lEQVRYCdVWzWoUQRCuqp2ICBLJXgITZL1EfQDBW/bkzUMUD7klD+ATSHBEfAIfQO+iXsWDxJsHL96EHAwhgzlkg8nBg25XWb0zIb0zs9muYYWkoKeru+vn664fBqElyZNuyh167NXJ8Ut8McjbmEraKHkd7uAnAFku+VWdb3reSmRV8PKSLfZ0Gjn3a6Xlcq9YGb6tADjn+lUfTXtVmaZ1KwBIvFI11rRXlWlatwIAAv2asaa9mlB9wwygiDX26qaw1yYPzFXg2N1GgG0FMF8Oj+VIx7E/03lHx8UhvYyNZLN7BwSPgekXXLribw7w5/c8EF+DBK5idvDVYtEEwMeYefjjLAdEyQ3M9nfOkgnPTEkYU+sxMq0BxNR6jExrAI31H1rzvLEfRIdgcv1XEdj6QTQAS2wtstEALLG1yEZ3QhH6oDX7ExBSFEkFINXH98NTrme5IOaaA7kIfiu2L8A3qhH9zRbukdCqdsA98TdElyeMe5BI8Rs2xHRIsoTSSVFfCFCWGPn9XHb4cdobRIWABNf0add9jakDjQJpJ1bTXOJXnnRXHRf+dNL1ZV1MBRCXhMbaHqGI1JkKIL7+i8uffuP6wVQAzO7+qVEbF6NbS0LJureYcWXUUhH66nLR5rYmva+2tjRFtojkM2aD76HEGAD3tPtKM309FJg5j/K682ywcWJ3PASCcycH/22u+Bh7Aa0ehM2Fu4z0SAE81HF9RkB21c5bEn4Dzw+/qNOyXr3DCTQDMBOdhi4nAgiFDGCinIa2owCEChUwD8qzd03PG+qdW/4fDzjUMcE1ZpIAAAAASUVORK5CYII=');
  }

  div.callout-caution.callout-style-default .callout-caption {
    background-color: #ffe5d0
  }

  </style>
  <style type="text/css">
    .reveal div.sourceCode {
      margin: 0;
      overflow: auto;
    }
    .reveal div.hanging-indent {
      margin-left: 1em;
      text-indent: -1em;
    }
    .reveal .slide:not(.center) {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide.scrollable {
      overflow-y: auto;
    }
    .reveal .footnotes {
      height: 100%;
      overflow-y: auto;
    }
    .reveal .slide .absolute {
      position: absolute;
      display: block;
    }
    .reveal .footnotes ol {
      counter-reset: ol;
      list-style-type: none; 
      margin-left: 0;
    }
    .reveal .footnotes ol li:before {
      counter-increment: ol;
      content: counter(ol) ". "; 
    }
    .reveal .footnotes ol li > p:first-child {
      display: inline-block;
    }
    .reveal .slide ul,
    .reveal .slide ol {
      margin-bottom: 0.5em;
    }
    .reveal .slide ul li,
    .reveal .slide ol li {
      margin-top: 0.4em;
      margin-bottom: 0.2em;
    }
    .reveal .slide ul[role="tablist"] li {
      margin-bottom: 0;
    }
    .reveal .slide ul li > *:first-child,
    .reveal .slide ol li > *:first-child {
      margin-block-start: 0;
    }
    .reveal .slide ul li > *:last-child,
    .reveal .slide ol li > *:last-child {
      margin-block-end: 0;
    }
    .reveal .slide .columns:nth-child(3) {
      margin-block-start: 0.8em;
    }
    .reveal blockquote {
      box-shadow: none;
    }
    .reveal .tippy-content>* {
      margin-top: 0.2em;
      margin-bottom: 0.7em;
    }
    .reveal .tippy-content>*:last-child {
      margin-bottom: 0.2em;
    }
    .reveal .slide > img.stretch.quarto-figure-center,
    .reveal .slide > img.r-stretch.quarto-figure-center {
      display: block;
      margin-left: auto;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-left,
    .reveal .slide > img.r-stretch.quarto-figure-left  {
      display: block;
      margin-left: 0;
      margin-right: auto; 
    }
    .reveal .slide > img.stretch.quarto-figure-right,
    .reveal .slide > img.r-stretch.quarto-figure-right  {
      display: block;
      margin-left: auto;
      margin-right: 0; 
    }
  </style>
</head>
<body class="quarto-light">
  <div class="reveal">
    <div class="slides">

<section id="title-slide" class="center">
  <h1 class="title">LIME and friends</h1>
  <p class="subtitle">eXplainable AI</p>
  <p class="author">Przemysław Biecek</p>
  <p class="date">Machine Learning @ MIMUW 2022</p>
</section>

<section class="slide level2">

<p>
<img src="images/aitech.png" width="100%">
</p>
</section>
<section>
<section id="paper-of-the-day" class="title-slide slide level1 center">
<h1>Paper of the day</h1>
<div class="cell">
<style type="text/css">
.reveal {
  font-size: 24px;
  line-height: 1.6!important;
}
code {
  font-size: 18px!important;
  line-height: 1.2!important;
}
pre {
  line-height: 1.2!important;
}
</style>
</div>
</section>
<section id="why-should-i-trust-you-explaining-the-predictions-of-any-classifier" class="slide level2">
<h2>“Why Should I Trust You?”: Explaining the Predictions of Any Classifier</h2>
<ul>
<li>In this course, you will learn about the XAI methods and tools, but also about selected papers and researchers.</li>
<li>Today we will talk about LIME method, so the article of the day will be the LIME paper from 2016: <a href="https://www.kdd.org/kdd2016/papers/files/rfp0573-ribeiroA.pdf">“Why Should I Trust You?” Explaining the Predictions of Any Classifier</a></li>
</ul>
<p>
</p><center>
<img src="images/lime_abstract.png" width="80%">
</center>
<p></p>
</section>
<section id="lime-paper-in-numbers" class="slide level2">
<h2>LIME paper in numbers</h2>
<ul>
<li>The LIME paper has over 10k citations</li>
<li>The <code>lime</code> python package has today over 10k start on github</li>
<li>The <em>husky example</em> is now the most frequently presented example calling for debugging models (more on this later)</li>
</ul>
<p>
<img src="images/lime_popular2.png" width="100%">
</p>
<p>
<img src="images/lime_popular3.png" width="100%">
</p>
</section>
<section id="why-lime" class="slide level2">
<h2>Why LIME?</h2>
<ul>
<li>Gives sparse explanations based on an interpretable data space</li>
<li>Very popular, especially for computer vision / NLP tasks</li>
<li>Very tempting approach – explain a complex model by a simpler surrogate (although intuition can be deceptive here)</li>
</ul>
<p>
<img src="images/shap_intro1.png" width="100%">
</p>
<p>
<img src="images/shap_intro2.png" width="100%">
</p>
<ul>
<li>Figures below are from the paper <a href="https://link.springer.com/chapter/10.1007/978-3-031-04083-2_2">Explainable AI Methods - A Brief Overview</a></li>
</ul>
</section>
<section id="xai-pyramid" class="slide level2">
<h2>XAI pyramid</h2>
<ul>
<li>Thinking about the XAI pyramid, we are still in the same group of solutions as SHAP, i.e.&nbsp;local explanations focused on the importance of features</li>
<li>As with SHAP, local LIME explanations can be used to explain the global model</li>
</ul>
<p>
<img src="images/XAI.png" width="100%">
</p>
</section>
<section id="xai-pyramid-1" class="slide level2">
<h2>XAI pyramid</h2>
<ul>
<li>LIME is based on one of the three fundamental approaches to explanation of predictive models.</li>
<li>LIME corresponds to panel B – approximation with linear surrogate model to get some understanding about black-box model behavior around <span class="math inline">\(x\)</span></li>
</ul>
<p>
<img src="images/xai_piramide_shap2.png" width="100%">
</p>
</section></section>
<section>
<section id="lime---local-interpretable-model-agnostic-explanations" class="title-slide slide level1 center">
<h1>LIME - Local Interpretable Model-agnostic Explanations</h1>

</section>
<section id="start-with-why" class="slide level2">
<h2>Start with Why</h2>
<p>Desired characteristics of explanations (from LIME paper)</p>
<ul>
<li>Explanations should be easy to undestand = interpretable (simple, sparse, based on interpretable features) for a user</li>
<li>Good explanation should be model-agnostic, i.e.&nbsp;does not depend on model structure. This will help to compare explanations for different models</li>
<li>Local fidelity of explanations</li>
</ul>
<p>
</p><center>
<img src="images/lime_intro.png" width="85%"><br> Explanation process. Figure from LIME paper
</center>
<p></p>
</section>
<section id="core-idea" class="slide level2">
<h2>Core idea</h2>
<p>The core ideas behind LIME are:</p>
<ul>
<li>Input to the model will be transformed into an interpretable feature space</li>
<li>Local model behaviour will be explained by approximating it by an interpretable surrogate model (e.g.&nbsp;a shallow tree or a linear regression model)</li>
<li>Local approximation is trained on artificial points generated from the neighborhood of the observation of interest <span class="math inline">\(x\)</span></li>
</ul>
<p>
</p><center>
<img src="images/lime_introduction.png" width="50%">
</center>
<br>Figure from EMA book
<p></p>
</section>
<section id="fidelity-interpretability-trade-off" class="slide level2">
<h2>Fidelity-Interpretability Trade-off</h2>
<p>The explanation will be a model <span class="math inline">\(g\)</span> that approximates the behavior of the complex model <span class="math inline">\(f\)</span> and is as simple as possible</p>
<p><span class="math display">\[
\hat g = \arg \min_{g \in G} L\{f, g, \pi(x)\} + \Omega(g)
\]</span></p>
<p>where</p>
<ul>
<li><span class="math inline">\(f()\)</span> is a model to be explained</li>
<li><span class="math inline">\(x\)</span> is an observation of interest</li>
<li><span class="math inline">\(G\)</span> is a class of interpretable models</li>
<li><span class="math inline">\(\hat g\)</span> is an explanation, a model from class <span class="math inline">\(G\)</span></li>
<li><span class="math inline">\(\Omega(g)\)</span> is a penalty function that measures complexity of models from <span class="math inline">\(G\)</span>. For regression models it could be the number of non-zero coefficients, for trees the number of nodes. For simplicity, we will consider a family of models <span class="math inline">\(G\)</span> such that all models in this family have complexity <span class="math inline">\(K\)</span></li>
<li><span class="math inline">\(L()\)</span> a function measuring the discrepancy between models <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> in the neighborhood <span class="math inline">\(\pi(x^*)\)</span></li>
</ul>
</section>
<section id="lime-algorithm" class="slide level2">
<h2>LIME Algorithm</h2>
<p>Explanations can be calculated with a following instructions.</p>
<ol type="1">
<li>Let <span class="math inline">\(x'\)</span> = <span class="math inline">\(h\)</span>(x) be a version of <span class="math inline">\(x\)</span> in the interpretable data space</li>
<li>for i in 1…N {</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;z’[i] = <code>sample_around</code>(x’)</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;y’[i] = <span class="math inline">\(f\)</span>(z’[i])</li>
<li>&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;w’[i] = <code>similarity</code>(x’, z’[i])</li>
<li>}</li>
<li>return <code>K-LASSO</code>(y’, x’, w’)</li>
</ol>
<p>where</p>
<ul>
<li><span class="math inline">\(x\)</span> – an observation to be explained</li>
<li><span class="math inline">\(N\)</span> – sample size needed to fit a glass-box model</li>
<li><span class="math inline">\(K\)</span> – complexity, the maximum number of variables in the glass-box model</li>
<li><code>similarity</code> – a distance function in the original data space</li>
<li><code>K-LASSO</code> – a weighted LASSO linear-regression model that selects K variables</li>
<li>w’ – weights that measure of the similarity between original observation <span class="math inline">\(x\)</span> and new artificially generated observations. Weights may be based on <span class="math inline">\(\exp(-d)\)</span> function, where <span class="math inline">\(d\)</span> is an Euclidean distance, cosine distance or other distance measure (depending on the data structure),</li>
</ul>
</section>
<section id="example-duck-or-horse-14" class="slide level2">
<h2>Example: Duck or horse? 1/4</h2>
<p>
</p><center>
<img src="images/lime_intro2.png" width="30%">
</center>
<p></p>
<p>Let’s see how LIME can be used to solve this problem.</p>
<p><strong>Initial settings</strong></p>
<ul>
<li>Let’s consider a VGG16 neural network trained on the ImageNet data</li>
<li>Input size are images 244 <span class="math inline">\(\times\)</span> 244 pixels. We have 1000 potential categories for the training data</li>
<li>The input space is of dimension 3 <span class="math inline">\(\times\)</span> 244 <span class="math inline">\(\times\)</span> 244, i.e.&nbsp;it is a 178 608-dimensional space</li>
<li>We need to translate the input to the interpretable data space, here image will be transformed into superpixels, which are treated as binary features (see an example later)</li>
<li>In this example <span class="math inline">\(f()\)</span> operates on space with <span class="math inline">\(178 608\)</span> dimensions, while the glass-box model <span class="math inline">\(g()\)</span> operates on a binary space with <span class="math inline">\(100\)</span> dimensions</li>
<li>We will ask for explanations of complexity 10</li>
</ul>
</section>
<section id="example-duck-or-horse-24" class="slide level2">
<h2>Example: Duck or horse? 2/4</h2>
<p><strong>Interpretable data space</strong></p>
<ul>
<li>Interpretable data space is a binary space that encodes presence or absence of selected features</li>
<li>The interpretable space can be constructed globally (e.g.&nbsp;for tabular data) or locally (e.g.&nbsp;for images)</li>
<li>For image data, the most common approach constructs an interpretable data space for each observation separately by using a segmentation algorithm.</li>
<li>The result is the division of the input image into a certain number of regions/called superpixels</li>
</ul>
<p>
<img src="images/lime_ex_1.png" width="100%">
</p>
</section>
<section id="example-duck-or-horse-34" class="slide level2">
<h2>Example: Duck or horse? 3/4</h2>
<p><strong>Sampling around x</strong></p>
<ul>
<li>We sample around the observation x’ in the interpretable space</li>
<li>Since it’s a binary space in which an observation <span class="math inline">\(x\)</span> is represented by a vector of ones</li>
<li>Sampling corresponds to selecting randomly coordinates that will be flipped to zero</li>
<li>We need N of such new observations</li>
</ul>
<p>
<img src="images/lime_ex_2.png" width="100%">
</p>
</section>
<section id="example-duck-or-horse-44" class="slide level2">
<h2>Example: Duck or horse? 4/4</h2>
<p><strong>Fitting of an interpretable model</strong></p>
<ul>
<li>For new data, we make predictions with model <span class="math inline">\(f()\)</span></li>
<li>And then for the observations in the interpretable representation we train a K-LASSO model which will have <span class="math inline">\(K\)</span> non-zero coefficients</li>
<li>We can use the <span class="math inline">\(R^2\)</span> coefficient to assess the quality of fit of the model <span class="math inline">\(g()\)</span></li>
</ul>
<p>
</p><center>
<img src="images/lime_ex_3.png" width="75%">
</center>
<p></p>
</section>
<section id="interpretable-data-representations" class="slide level2">
<h2>Interpretable data representations</h2>
<p>How to transform the input data into a binary vector of shorter length?</p>
<ul>
<li>For image data interpretable feature space is commonly based on superpixels, i.e.&nbsp;through image segmentation</li>
<li>For text data, words or groups of words are frequently used as interpretable variables</li>
<li>For tabular data, continuous variables are often discretized to obtain interpretable bianary variables. In the case of categorical variables, combination of levels is used to get bianary variables.</li>
</ul>
<p>
</p><center>
<img src="images/lime_ex_8.png" width="75%">
</center>
<br> Example from <a href="https://marcotcr.github.io/lime/tutorials/Lime%20-%20basic%20usage%2C%20two%20class%20case.html">LIME github</a>
<p></p>
</section>
<section id="model-debugging-13" class="slide level2">
<h2>Model debugging 1/3</h2>
<ul>
<li>There are many reasons to know and develop XAI techniques</li>
<li>One of them is the ability to debug the model</li>
<li>The most well-known example is improving the performance of a network that misclassified the following image</li>
<li>How LIME can help here?</li>
</ul>
<p>
</p><center>
<img src="images/lime_ex_9.png" width="25%">
</center>
<br> Figure from <a href="http://www.facweb.iitkgp.ac.in/~niloy/COURSE/Spring2018/IntelligentSystem/PPT_2018/why_should_i_trust_ppt.pdf">presentation about LIME</a> by Sameer Singh
<p></p>
</section>
<section id="model-debugging-23" class="slide level2">
<h2>Model debugging 2/3</h2>
<ul>
<li>The model works very well. Classification between husky of wolf in accurate in almost every image except one. Why?</li>
</ul>
<p>
</p><center>
<img src="images/lime_ex_6.png" width="70%">
</center>
<br> Figure from <a href="http://www.facweb.iitkgp.ac.in/~niloy/COURSE/Spring2018/IntelligentSystem/PPT_2018/why_should_i_trust_ppt.pdf">presentation about LIME</a> by Sameer Singh
<p></p>
</section>
<section id="model-debugging-33" class="slide level2">
<h2>Model debugging 3/3</h2>
<ul>
<li>Can LIME’s explanation help us find the source of the problem?</li>
<li>It turns out that in the case of classification as a wolf, the important feature is the snow in the background</li>
<li>Effectively, the model has learned to recognize snow in the background and so classifies as a wolf class</li>
<li>This is not a feature that people use for classification wolf/husky. But would you sacrifice the quality of the model to remove the dependence on using the background for classification?</li>
</ul>
<p>
</p><center>
<img src="images/lime_ex_5.png" width="100%">
</center>
<br> Figure from <a href="http://www.facweb.iitkgp.ac.in/~niloy/COURSE/Spring2018/IntelligentSystem/PPT_2018/why_should_i_trust_ppt.pdf">presentation about LIME</a> by Sameer Singh
<p></p>
<ul>
<li>This story has a happy ending. Proper training that cancelled the model dependency on the snow feature improved the accuracy of the model</li>
</ul>
</section></section>
<section>
<section id="from-local-to-global" class="title-slide slide level1 center">
<h1>From Local to Global</h1>

</section>
<section id="explaining-through-examples" class="slide level2">
<h2>Explaining through examples</h2>
<p>The LIME method was designed to explain the model’s behavior locally, around the observation of interest. But we are often interested in knowing or at least getting an intuition about how the model works globally.</p>
<p>The LIME paper proposes two approaches to globalizing LIME. Both are based on selecting some subset of observations that will be fairly representative of the entire dataset. Assuming the user has time to look at LIME explanations for B observations, the question is how to select them.</p>
<p><strong>Submodular pick (SP) algorithm</strong></p>
<p>
</p><center>
<img src="images/lime_pick.png" width="75%">
</center>
<p></p>
<p>Criterion for selecting observations for global explanations</p>
<p><span class="math display">\[
c(V, W, I) = \sum_{j \in P'} 1_{\exists i\in V; W_{i,j} \neq 0} I_j
\]</span></p>
<p>where <span class="math inline">\(I_j\)</span> is feature importance for feature <span class="math inline">\(i\)</span> while <span class="math inline">\(P'\)</span> is a set of features in an interpretable data space.</p>
<p>The LIME paper presents a user-study example where the submodular picks method most effectively convinces the user how the model works.</p>
</section>
<section id="can-non-experts-improve-a-classifier" class="slide level2">
<h2>Can non-experts improve a classifier?</h2>
<ul>
<li>The LIME paper describes the results of several experiments involving humans subjects</li>
<li>Very interesting results involved using explanations to improve the model, even if the improvement is generated by the knowledge and actions of non-ML-experts</li>
<li>The experiment was based on a model for a classification task based on text data</li>
<li>The explanations of the model generated by the LIME method were then shown to the participants of the experiment. That is, for each observation, the relevant words were highlighted</li>
<li>Participants could determine that some of these words were artifacts and should not be used by the model</li>
<li>The model was then trained again on the remaining features, with the artifacts removed</li>
<li>It turns out that such feature engineering using experts led to better results after several rounds</li>
</ul>
<p>
</p><center>
<img src="images/lime_fe.png" width="60%">
</center>
<br>Figure from the LIME paper
<p></p>
</section></section>
<section>
<section id="anchors" class="title-slide slide level1 center">
<h1>Anchors</h1>

</section>
<section id="anchors-high-precision-model-agnostic-explanations-13" class="slide level2">
<h2>Anchors: High-Precision Model-Agnostic Explanations 1/3</h2>
<ul>
<li>A limitation of the LIME method is the assumption that locally the behavior of a complex model can be explained by approximating it with an additive linear regression model.</li>
<li>But if there are significant interactions in the model then a local approximation with an additive model will not reflect the model’s behavior well.</li>
<li>In the example below, we can see that the word ‘not’ has no additive effect, so its effect does not additively compound with the word ‘bad’.</li>
<li>To explain the model’s behavior, we need a rule that contains both variables.</li>
</ul>
<p>
</p><center>
<img src="images/lime_anchors.png" width="60%">
</center>
<br> Figure from the <a href="https://homes.cs.washington.edu/~marcotcr/aaai18.pdf">Anchors</a> paper
<p></p>
</section>
<section id="anchors-high-precision-model-agnostic-explanations-23" class="slide level2">
<h2>Anchors: High-Precision Model-Agnostic Explanations 2/3</h2>
<ul>
<li><p>In <a href="https://homes.cs.washington.edu/~marcotcr/aaai18.pdf">Anchors: High-Precision Model-Agnostic Explanations</a>, the authors present an alternative explanation method that finds the shortest subset of conditions sufficient to explain the local behavior of the model.</p></li>
<li><p>The intuition behind the anchor is as follows: for a given observation anchors are <em>“sufficient”</em> conditions for a model prediction.</p></li>
<li><p>More formally: <span class="math inline">\(A\)</span> is a rule = set of logical conditions. We will say that <span class="math inline">\(A\)</span> is an <em>anchor</em> if <span class="math inline">\(A(x) = 1\)</span>, i.e.&nbsp;observation <span class="math inline">\(x\)</span> fulfill all these conditions and probability that rule <span class="math inline">\(A\)</span> is true around <span class="math inline">\(x\)</span> is higher than <span class="math inline">\(\tau\)</span>, i.e.</p></li>
</ul>
<p><span class="math display">\[
E_{D(z|A)}[1_{f(x) = f(z)}] \geq \tau,
\]</span></p>
<p>where <span class="math inline">\(D(z|A)\)</span> is a distribution of points that fulfill the rule <span class="math inline">\(A\)</span>.</p>
<p>
</p><center>
<img src="images/lime_anchors2.png" width="60%">
</center>
<br> Figure from the <a href="https://homes.cs.washington.edu/~marcotcr/aaai18.pdf">Anchors</a> paper
<p></p>
</section>
<section id="anchors-high-precision-model-agnostic-explanations-33" class="slide level2">
<h2>Anchors: High-Precision Model-Agnostic Explanations 3/3</h2>
<ul>
<li>,,sufficient’’ explanation</li>
<li>If these segments are in the picture then it is classified as a beagle</li>
</ul>
<p>
</p><center>
<img src="images/anchors_1.png" width="90%">
</center>
<br> Figure from the <a href="https://homes.cs.washington.edu/~marcotcr/aaai18.pdf">Anchors</a> paper
<p></p>
</section></section>
<section>
<section id="lore" class="title-slide slide level1 center">
<h1>LORE</h1>

</section>
<section id="lore-local-rule-based-explanations" class="slide level2">
<h2>LORE: Local Rule-Based Explanations</h2>
<p>There are many more explanations that analyze the local neighborhood around a point <span class="math inline">\(x\)</span>. An interesting variant is the LORE method for local explanations that uses few interesting concepts, such as counterfactual explanations.</p>
<p>The way LORE explanations are constructed is the following:</p>
<ol type="1">
<li>Use a genetic algorithm to construct a balanced set of points around <span class="math inline">\(x\)</span>. In this set we have as many representatives of class <span class="math inline">\(1\)</span> as <span class="math inline">\(0\)</span></li>
<li>Based on this data, a decision tree is trained</li>
<li>Two types of rules are drawn from this decision tree. (1) an explanation, that is, a rule corresponding to the path for the observation <span class="math inline">\(x\)</span> and (2) a counterfactual explanation, that is, a set of rules saying what needs to be changed in <span class="math inline">\(x\)</span> so that the predicted class is not <span class="math inline">\(f(x)\)</span></li>
</ol>
<p>
</p><center>
<img src="images/lore_ex_1.png" width="80%">
</center>
<br> Figure from the <a href="https://arxiv.org/pdf/1805.10820.pdf">LORE</a> paper
<p></p>
</section></section>
<section id="take-home-message" class="title-slide slide level1 center">
<h1>Take-home message</h1>
<ul>
<li>LIME method explains local model behavior with <strong>interpretable linear surrogate model</strong></li>
<li>LIME generates <strong>sparse explanations</strong>, with K features</li>
<li>Explanations use an <strong>interpretable feature space</strong>, like superpixels for image data, tokens for NLP, quartiles for numeric tabular data</li>
<li>Sampling strategy is very important. In LIME sampling operates on binary interpretable features space. In LORE sampling results in balanced data.</li>
<li><strong>From local to global</strong>. Based on local explanations, global explanations can be constructed</li>
</ul>
</section>

<section id="code-examples" class="title-slide slide level1 center">
<h1>Code-examples</h1>
<ul>
<li>See <a href="https://github.com/mim-uw/eXplainableMachineLearning-2023/Materials">Materials at GitHub</a></li>
</ul>
<p>
<img src="images/lime_code.png" width="100%">
</p>
<p><img src="images/XAI.png" class="slide-logo"></p>
<div class="footer footer-default">
<p>eXplainable AI – LIME and friends – MIM UW – 2022</p>
</div>
</section>
    </div>
  </div>

  <script>window.backupDefine = window.define; window.define = undefined;</script>
  <script src="03_lime_files/libs/revealjs/dist/reveal.js"></script>
  <!-- reveal.js plugins -->
  <script src="03_lime_files/libs/revealjs/plugin/quarto-line-highlight/line-highlight.js"></script>
  <script src="03_lime_files/libs/revealjs/plugin/pdf-export/pdfexport.js"></script>
  <script src="03_lime_files/libs/revealjs/plugin/reveal-menu/menu.js"></script>
  <script src="03_lime_files/libs/revealjs/plugin/reveal-menu/quarto-menu.js"></script>
  <script src="03_lime_files/libs/revealjs/plugin/reveal-chalkboard/plugin.js"></script>
  <script src="03_lime_files/libs/revealjs/plugin/quarto-support/support.js"></script>
  

  <script src="03_lime_files/libs/revealjs/plugin/notes/notes.js"></script>
  <script src="03_lime_files/libs/revealjs/plugin/search/search.js"></script>
  <script src="03_lime_files/libs/revealjs/plugin/zoom/zoom.js"></script>
  <script src="03_lime_files/libs/revealjs/plugin/math/math.js"></script>
  <script>window.define = window.backupDefine; window.backupDefine = undefined;</script>

  <script>

      // Full list of configuration options available at:
      // https://revealjs.com/config/
      Reveal.initialize({
'controlsAuto': true,
'previewLinksAuto': false,
'smaller': false,
'pdfSeparateFragments': false,
'autoAnimateEasing': "ease",
'autoAnimateDuration': 1,
'autoAnimateUnmatched': true,
'menu': {"side":"left","useTextContentForMissingTitles":true,"markers":false,"loadIcons":false,"custom":[{"title":"Tools","icon":"<i class=\"fas fa-gear\"></i>","content":"<ul class=\"slide-menu-items\">\n<li class=\"slide-tool-item active\" data-item=\"0\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.fullscreen(event)\"><kbd>f</kbd> Fullscreen</a></li>\n<li class=\"slide-tool-item\" data-item=\"1\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.speakerMode(event)\"><kbd>s</kbd> Speaker View</a></li>\n<li class=\"slide-tool-item\" data-item=\"2\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>o</kbd> Slide Overview</a></li>\n<li class=\"slide-tool-item\" data-item=\"3\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.overview(event)\"><kbd>e</kbd> PDF Export Mode</a></li>\n<li class=\"slide-tool-item\" data-item=\"4\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleChalkboard(event)\"><kbd>b</kbd> Toggle Chalkboard</a></li>\n<li class=\"slide-tool-item\" data-item=\"5\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.toggleNotesCanvas(event)\"><kbd>c</kbd> Toggle Notes Canvas</a></li>\n<li class=\"slide-tool-item\" data-item=\"6\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.downloadDrawings(event)\"><kbd>d</kbd> Download Drawings</a></li>\n<li class=\"slide-tool-item\" data-item=\"7\"><a href=\"#\" onclick=\"RevealMenuToolHandlers.keyboardHelp(event)\"><kbd>?</kbd> Keyboard Help</a></li>\n</ul>"}],"openButton":true},
'chalkboard': {"buttons":false},
'smaller': false,
 
        // Display controls in the bottom right corner
        controls: false,

        // Help the user learn the controls by providing hints, for example by
        // bouncing the down arrow when they first encounter a vertical slide
        controlsTutorial: false,

        // Determines where controls appear, "edges" or "bottom-right"
        controlsLayout: 'edges',

        // Visibility rule for backwards navigation arrows; "faded", "hidden"
        // or "visible"
        controlsBackArrows: 'faded',

        // Display a presentation progress bar
        progress: true,

        // Display the page number of the current slide
        slideNumber: 'c/t',

        // 'all', 'print', or 'speaker'
        showSlideNumber: 'all',

        // Add the current slide number to the URL hash so that reloading the
        // page/copying the URL will return you to the same slide
        hash: true,

        // Start with 1 for the hash rather than 0
        hashOneBasedIndex: false,

        // Flags if we should monitor the hash and change slides accordingly
        respondToHashChanges: true,

        // Push each slide change to the browser history
        history: true,

        // Enable keyboard shortcuts for navigation
        keyboard: true,

        // Enable the slide overview mode
        overview: true,

        // Disables the default reveal.js slide layout (scaling and centering)
        // so that you can use custom CSS layout
        disableLayout: false,

        // Vertical centering of slides
        center: false,

        // Enables touch navigation on devices with touch input
        touch: true,

        // Loop the presentation
        loop: false,

        // Change the presentation direction to be RTL
        rtl: false,

        // see https://revealjs.com/vertical-slides/#navigation-mode
        navigationMode: 'linear',

        // Randomizes the order of slides each time the presentation loads
        shuffle: false,

        // Turns fragments on and off globally
        fragments: true,

        // Flags whether to include the current fragment in the URL,
        // so that reloading brings you to the same fragment position
        fragmentInURL: false,

        // Flags if the presentation is running in an embedded mode,
        // i.e. contained within a limited portion of the screen
        embedded: false,

        // Flags if we should show a help overlay when the questionmark
        // key is pressed
        help: true,

        // Flags if it should be possible to pause the presentation (blackout)
        pause: true,

        // Flags if speaker notes should be visible to all viewers
        showNotes: false,

        // Global override for autoplaying embedded media (null/true/false)
        autoPlayMedia: null,

        // Global override for preloading lazy-loaded iframes (null/true/false)
        preloadIframes: null,

        // Number of milliseconds between automatically proceeding to the
        // next slide, disabled when set to 0, this value can be overwritten
        // by using a data-autoslide attribute on your slides
        autoSlide: 0,

        // Stop auto-sliding after user input
        autoSlideStoppable: true,

        // Use this method for navigation when auto-sliding
        autoSlideMethod: null,

        // Specify the average time in seconds that you think you will spend
        // presenting each slide. This is used to show a pacing timer in the
        // speaker view
        defaultTiming: null,

        // Enable slide navigation via mouse wheel
        mouseWheel: false,

        // The display mode that will be used to show slides
        display: 'block',

        // Hide cursor if inactive
        hideInactiveCursor: true,

        // Time before the cursor is hidden (in ms)
        hideCursorTime: 5000,

        // Opens links in an iframe preview overlay
        previewLinks: false,

        // Transition style (none/fade/slide/convex/concave/zoom)
        transition: 'none',

        // Transition speed (default/fast/slow)
        transitionSpeed: 'default',

        // Transition style for full page slide backgrounds
        // (none/fade/slide/convex/concave/zoom)
        backgroundTransition: 'none',

        // Number of slides away from the current that are visible
        viewDistance: 3,

        // Number of slides away from the current that are visible on mobile
        // devices. It is advisable to set this to a lower number than
        // viewDistance in order to save resources.
        mobileViewDistance: 2,

        // The "normal" size of the presentation, aspect ratio will be preserved
        // when the presentation is scaled to fit different resolutions. Can be
        // specified using percentage units.
        width: 1050,

        height: 700,

        // Factor of the display size that should remain empty around the content
        margin: 0.1,

        math: {
          mathjax: 'https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js',
          config: 'TeX-AMS_HTML-full',
          tex2jax: {
            inlineMath: [['\\(','\\)']],
            displayMath: [['\\[','\\]']],
            balanceBraces: true,
            processEscapes: false,
            processRefs: true,
            processEnvironments: true,
            preview: 'TeX',
            skipTags: ['script','noscript','style','textarea','pre','code'],
            ignoreClass: 'tex2jax_ignore',
            processClass: 'tex2jax_process'
          },
        },

        // reveal.js plugins
        plugins: [QuartoLineHighlight, PdfExport, RevealMenu, RevealChalkboard, QuartoSupport,

          RevealMath,
          RevealNotes,
          RevealSearch,
          RevealZoom
        ]
      });
    </script>
    
    <script>
      // htmlwidgets need to know to resize themselves when slides are shown/hidden.
      // Fire the "slideenter" event (handled by htmlwidgets.js) when the current
      // slide changes (different for each slide format).
      (function () {
        function fireSlideChanged(previousSlide, currentSlide) {

          // dispatch for htmlwidgets
          const event = window.document.createEvent("Event");
          event.initEvent("slideenter", true, true);
          window.document.dispatchEvent(event);

          // dispatch for shiny
          if (window.jQuery) {
            if (previousSlide) {
              window.jQuery(previousSlide).trigger("hidden");
            }
            if (currentSlide) {
              window.jQuery(currentSlide).trigger("shown");
            }
          }
        }

        // hookup for reveal
        if (window.Reveal) {
          window.Reveal.addEventListener("slidechanged", function(event) {
            fireSlideChanged(event.previousSlide, event.currentSlide);
          });
        }

        // hookup for slidy
        if (window.w3c_slidy) {
          window.w3c_slidy.add_observer(function (slide_num) {
            // slide_num starts at position 1
            fireSlideChanged(null, w3c_slidy.slides[slide_num - 1]);
          });
        }

      })();
    </script>

    <script id="quarto-html-after-body" type="application/javascript">
    window.document.addEventListener("DOMContentLoaded", function (event) {
      const toggleBodyColorMode = (bsSheetEl) => {
        const mode = bsSheetEl.getAttribute("data-mode");
        const bodyEl = window.document.querySelector("body");
        if (mode === "dark") {
          bodyEl.classList.add("quarto-dark");
          bodyEl.classList.remove("quarto-light");
        } else {
          bodyEl.classList.add("quarto-light");
          bodyEl.classList.remove("quarto-dark");
        }
      }
      const toggleBodyColorPrimary = () => {
        const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
        if (bsSheetEl) {
          toggleBodyColorMode(bsSheetEl);
        }
      }
      toggleBodyColorPrimary();  
      const tabsets =  window.document.querySelectorAll(".panel-tabset-tabby")
      tabsets.forEach(function(tabset) {
        const tabby = new Tabby('#' + tabset.id);
      });
      const clipboard = new window.ClipboardJS('.code-copy-button', {
        target: function(trigger) {
          return trigger.previousElementSibling;
        }
      });
      clipboard.on('success', function(e) {
        // button target
        const button = e.trigger;
        // don't keep focus
        button.blur();
        // flash "checked"
        button.classList.add('code-copy-button-checked');
        var currentTitle = button.getAttribute("title");
        button.setAttribute("title", "Copied!");
        setTimeout(function() {
          button.setAttribute("title", currentTitle);
          button.classList.remove('code-copy-button-checked');
        }, 1000);
        // clear code selection
        e.clearSelection();
      });
      function tippyHover(el, contentFn) {
        const config = {
          allowHTML: true,
          content: contentFn,
          maxWidth: 500,
          delay: 100,
          arrow: false,
          appendTo: function(el) {
              return el.closest('section.slide') || el.parentElement;
          },
          interactive: true,
          interactiveBorder: 10,
          theme: 'quarto-reveal',
          placement: 'bottom-start'
        };
          config['offset'] = [0,0];
          config['maxWidth'] = 700;
        window.tippy(el, config); 
      }
      const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
      for (var i=0; i<noterefs.length; i++) {
        const ref = noterefs[i];
        tippyHover(ref, function() {
          let href = ref.getAttribute('href');
          try { href = new URL(href).hash; } catch {}
          const id = href.replace(/^#\/?/, "");
          const note = window.document.getElementById(id);
          return note.innerHTML;
        });
      }
      var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
      for (var i=0; i<bibliorefs.length; i++) {
        const ref = bibliorefs[i];
        const cites = ref.parentNode.getAttribute('data-cites').split(' ');
        tippyHover(ref, function() {
          var popup = window.document.createElement('div');
          cites.forEach(function(cite) {
            var citeDiv = window.document.createElement('div');
            citeDiv.classList.add('hanging-indent');
            citeDiv.classList.add('csl-entry');
            var biblioDiv = window.document.getElementById('ref-' + cite);
            if (biblioDiv) {
              citeDiv.innerHTML = biblioDiv.innerHTML;
            }
            popup.appendChild(citeDiv);
          });
          return popup.innerHTML;
        });
      }
    });
    </script>
    

</body></html>