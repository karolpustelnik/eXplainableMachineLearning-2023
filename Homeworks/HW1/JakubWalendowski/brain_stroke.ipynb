{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Brain stroke predictions\n",
    "I will analyze the brain stroke data set from Kaggle (https://www.kaggle.com/datasets/jillanisofttech/brain-stroke-dataset). I will compare the performance of Logistic Regression and Random Forest."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"brain_stroke.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data analysis\n",
    "Data consists of 10 features and 1 column of labels (output). Dimension of the data is: (4981,11). The features are:\n",
    "1) gender: \"Male\", \"Female\" or \"Other\"\n",
    "2) age: age of the patient\n",
    "3) hypertension: 0 if the patient doesn't have hypertension, 1 if the patient has hypertension\n",
    "4) heart disease: 0 if the patient doesn't have any heart diseases, 1 if the patient has a heart disease 5) ever-married: \"No\" or \"Yes\"\n",
    "6) worktype: \"children\", \"Govtjov\", \"Neverworked\", \"Private\" or \"Self-employed\" 7) Residencetype: \"Rural\" or \"Urban\"\n",
    "8) avgglucoselevel: average glucose level in blood\n",
    "9) bmi: body mass index\n",
    "10) smoking_status: \"formerly smoked\", \"never smoked\", \"smokes\" or \"Unknown\"*\n",
    "\n",
    "*Note: \"Unknown\" in smoking_status means that the information is unavailable for this patient"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 4981 entries, 0 to 4980\n",
      "Data columns (total 11 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   gender             4981 non-null   object \n",
      " 1   age                4981 non-null   float64\n",
      " 2   hypertension       4981 non-null   int64  \n",
      " 3   heart_disease      4981 non-null   int64  \n",
      " 4   ever_married       4981 non-null   object \n",
      " 5   work_type          4981 non-null   object \n",
      " 6   Residence_type     4981 non-null   object \n",
      " 7   avg_glucose_level  4981 non-null   float64\n",
      " 8   bmi                4981 non-null   float64\n",
      " 9   smoking_status     4981 non-null   object \n",
      " 10  stroke             4981 non-null   int64  \n",
      "dtypes: float64(3), int64(3), object(5)\n",
      "memory usage: 428.2+ KB\n",
      "None\n",
      "gender               0\n",
      "age                  0\n",
      "hypertension         0\n",
      "heart_disease        0\n",
      "ever_married         0\n",
      "work_type            0\n",
      "Residence_type       0\n",
      "avg_glucose_level    0\n",
      "bmi                  0\n",
      "smoking_status       0\n",
      "stroke               0\n",
      "dtype: int64\n",
      "   gender   age  hypertension  heart_disease ever_married      work_type  \\\n",
      "0    Male  67.0             0              1          Yes        Private   \n",
      "1    Male  80.0             0              1          Yes        Private   \n",
      "2  Female  49.0             0              0          Yes        Private   \n",
      "3  Female  79.0             1              0          Yes  Self-employed   \n",
      "4    Male  81.0             0              0          Yes        Private   \n",
      "\n",
      "  Residence_type  avg_glucose_level   bmi   smoking_status  stroke  \n",
      "0          Urban             228.69  36.6  formerly smoked       1  \n",
      "1          Rural             105.92  32.5     never smoked       1  \n",
      "2          Urban             171.23  34.4           smokes       1  \n",
      "3          Rural             174.12  24.0     never smoked       1  \n",
      "4          Urban             186.21  29.0  formerly smoked       1  \n"
     ]
    }
   ],
   "source": [
    "print(df.info())\n",
    "print(df.isnull().sum())\n",
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "There is no missing data in the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data balance\n",
    "The data is very unbalanced. This will have to be taken into account when training the model. To deal with this problem I will use oversampling technic."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [
    {
     "data": {
      "text/plain": "stroke\n0    4733\n1     248\ndtype: int64"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.groupby([\"stroke\"]).size()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Data preparation\n",
    "There are only two values for Residence_type, ever_married and gender columns. These columns can be easily encoded into 0 and 1. Data about the work_type can be encoded with one hot encoding, and the smoking_data looks useless, because there is a large part of the unknown fields, so I decided not to include this data in my model. Due to the use of linear regression, I decided to scale the age, avg_glucose_level and bmi data.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Residence_type\n",
      "Rural    2449\n",
      "Urban    2532\n",
      "dtype: int64\n",
      "ever_married\n",
      "No     1701\n",
      "Yes    3280\n",
      "dtype: int64\n",
      "gender\n",
      "Female    2907\n",
      "Male      2074\n",
      "dtype: int64\n",
      "work_type\n",
      "Govt_job          644\n",
      "Private          2860\n",
      "Self-employed     804\n",
      "children          673\n",
      "dtype: int64\n",
      "smoking_status   stroke\n",
      "Unknown          0         1453\n",
      "                 1           47\n",
      "formerly smoked  0          797\n",
      "                 1           70\n",
      "never smoked     0         1749\n",
      "                 1           89\n",
      "smokes           0          734\n",
      "                 1           42\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.groupby([\"Residence_type\"]).size())\n",
    "print(df.groupby([\"ever_married\"]).size())\n",
    "print(df.groupby([\"gender\"]).size())\n",
    "print(df.groupby([\"work_type\"]).size())\n",
    "print(df.groupby([\"smoking_status\", \"stroke\"]).size())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   gender       age  hypertension  heart_disease  ever_married  \\\n",
      "0     1.0  0.816895             0              1           1.0   \n",
      "1     1.0  0.975586             0              1           1.0   \n",
      "2     0.0  0.597168             0              0           1.0   \n",
      "3     0.0  0.963379             1              0           1.0   \n",
      "4     1.0  0.987793             0              0           1.0   \n",
      "\n",
      "   Residence_type  avg_glucose_level       bmi  stroke  work_type_Govt_job  \\\n",
      "0             1.0           0.801265  0.647564       1                   0   \n",
      "1             0.0           0.234512  0.530086       1                   0   \n",
      "2             1.0           0.536008  0.584527       1                   0   \n",
      "3             0.0           0.549349  0.286533       1                   0   \n",
      "4             1.0           0.605161  0.429799       1                   0   \n",
      "\n",
      "   work_type_Private  work_type_Self-employed  work_type_children  \n",
      "0                  1                        0                   0  \n",
      "1                  1                        0                   0  \n",
      "2                  1                        0                   0  \n",
      "3                  0                        1                   0  \n",
      "4                  1                        0                   0  \n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import minmax_scale, OrdinalEncoder\n",
    "enc = OrdinalEncoder()\n",
    "df[[\"Residence_type\", \"ever_married\", \"gender\"]] = enc.fit_transform(df[[\"Residence_type\", \"ever_married\", \"gender\"]])\n",
    "df = pd.get_dummies(df, columns=[\"work_type\"])\n",
    "df = df.drop(columns=[\"smoking_status\"])\n",
    "df[['age','avg_glucose_level','bmi']] = minmax_scale(df[['age','avg_glucose_level','bmi']])\n",
    "print(df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Train test split\n",
    "I will use train_test_split from sklearn.model_selection to split the data into 75% of the data for training and 25% for testing. I will use random_state=5 to make the results reproducible."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop(\"stroke\", axis=1)\n",
    "y = df[\"stroke\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, shuffle=True, random_state=5)\n",
    "from imblearn.over_sampling import SMOTE\n",
    "oversample = SMOTE()\n",
    "X_train, y_train = oversample.fit_resample(X_train, y_train)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Logistic regression\n",
    "Unfortunately, linear regression is not very precise about people who have a stroke (Only 13% on test dataset).\n",
    "But the recall (metric that quantifies the number of correct positive predictions made out of all positive predictions that could have been made) results are at the good level (75%), which is satisfactory for this type of classification. In this case it is better to have more false positives (poor precision) than many false negatives."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.75      0.85      1182\n",
      "           1       0.14      0.75      0.23        64\n",
      "\n",
      "    accuracy                           0.75      1246\n",
      "   macro avg       0.56      0.75      0.54      1246\n",
      "weighted avg       0.94      0.75      0.82      1246\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.81      0.74      0.77      3551\n",
      "           1       0.76      0.83      0.79      3551\n",
      "\n",
      "    accuracy                           0.78      7102\n",
      "   macro avg       0.79      0.78      0.78      7102\n",
      "weighted avg       0.79      0.78      0.78      7102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "lr = LogisticRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "y_pred = lr.predict(X_test)\n",
    "y_pred_train = lr.predict(X_train)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(classification_report(y_train, y_pred_train))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Random Forest Classifier\n",
    "The max_depth of the tree had to be significantly reduced to avoid overfitting. The tree performs a bit better than linear regression. Precision is similar, but recall has increased recall by a few percent."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.70      0.82      1182\n",
      "           1       0.13      0.81      0.22        64\n",
      "\n",
      "    accuracy                           0.71      1246\n",
      "   macro avg       0.56      0.76      0.52      1246\n",
      "weighted avg       0.94      0.71      0.79      1246\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.69      0.78      3551\n",
      "           1       0.75      0.92      0.82      3551\n",
      "\n",
      "    accuracy                           0.80      7102\n",
      "   macro avg       0.82      0.80      0.80      7102\n",
      "weighted avg       0.82      0.80      0.80      7102\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfc = RandomForestClassifier(max_depth=4, random_state=0)\n",
    "rfc.fit(X_train, y_train)\n",
    "y_pred = rfc.predict(X_test)\n",
    "y_pred_train = rfc.predict(X_train)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(classification_report(y_train, y_pred_train))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Summary\n",
    "In this notebook, I analyzed the brain stroke dataset. I created a logistic regression model and a random forest model. I used oversampling method to deal with the unbalanced data. To prevent overfitting in random forest classifier I limited the death of the tree. I used default classification_report from sklearn.metrics with  precision, recall F1 score and support metrics to evaluate the models."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
