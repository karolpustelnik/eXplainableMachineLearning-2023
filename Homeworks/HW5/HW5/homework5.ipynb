{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation-based variable importance (PVI) - Homework 5 - Karol Pustelnik\n",
    "\n",
    "In this notebook, we will focus on the permutation-based variable importance (PVI) method. We will use the PVI method to analyze the importance of the features in the heart disease dataset. We will compare results across many models and discuss the differences and similarities. I will mainly focus on one model - xgboost - with different settings of hyperparameters and variable transformations. We will also discuss the differences between the PVI and the SHAP tree method.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Theoretical background\n",
    "Permutation-based variable importance (PVI)\n",
    "\n",
    "\n",
    "Permutation feature importance is a model inspection technique that can be used for any fitted estimator when the data is tabular. This is especially useful for non-linear or opaque estimators. The permutation feature importance is defined to be the decrease in a model score when a single feature value is randomly shuffled. This procedure breaks the relationship between the feature and the target, thus the drop in the model score is indicative of how much the model depends on the feature. This technique benefits from being `model agnostic` and can be calculated many times with different permutations of the feature.\n",
    "\n",
    "source: https://scikit-learn.org/stable/modules/permutation_importance.html\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dalex as dx\n",
    "import xgboost\n",
    "import shap\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import alibi\n",
    "import plotly.express as px\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import plotly.io as pio\n",
    "pio.renderers.default = \"notebook\"\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data\n",
    "data = pd.read_csv('heart.csv')\n",
    "data_org = pd.read_csv('heart.csv')\n",
    "\n",
    "# One hot encoding of categorical features\n",
    "\n",
    "#data = pd.get_dummies(data, columns = ['sex', 'cp', 'fbs', 'restecg', 'exng', 'slp', 'caa', 'thall'])\n",
    "\n",
    "# Splitting the data\n",
    "\n",
    "y = data['output'] # Target variable\n",
    "X = data.drop(['output'], axis=1) # Features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Xgboost\n",
    "model = xgboost.XGBClassifier(\n",
    "    n_estimators=200, \n",
    "    max_depth=4, \n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Logistic regression\n",
    "log_reg = LogisticRegression() \n",
    "log_reg.fit(X_train, y_train) # Fitting model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pf_xgboost_classifier_categorical(model, df):\n",
    "    df.loc[:, df.dtypes == 'object'] =\\\n",
    "        df.select_dtypes(['object'])\\\n",
    "        .apply(lambda x: x.astype('category'))\n",
    "    return model.predict_proba(df)[:, 1]\n",
    "\n",
    "explainer = dx.Explainer(model, X_test, y_test, predict_function=pf_xgboost_classifier_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>recall</th>\n",
       "      <th>precision</th>\n",
       "      <th>f1</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>auc</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>XGBClassifier</th>\n",
       "      <td>0.78125</td>\n",
       "      <td>0.862069</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.819672</td>\n",
       "      <td>0.912716</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "<dalex.model_explanations._model_performance.object.ModelPerformance at 0x281d5beb0>"
      ]
     },
     "execution_count": 271,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "explainer.model_performance()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Permutation-based Variable Importance for xgboost\n",
    "\n",
    "I will now calculate the permutation-based variable importance for the xgboost model. The settings are as follows:\n",
    "\n",
    "* xgboost model\n",
    "* data without transformations\n",
    "* n_estimators = 200\n",
    "* max_depth = 4\n",
    "* the rest of the parameters are the default\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvi = explainer.model_parts(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>dropout_loss</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chol</td>\n",
       "      <td>0.076509</td>\n",
       "      <td>XGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>exng</td>\n",
       "      <td>0.079095</td>\n",
       "      <td>XGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>oldpeak</td>\n",
       "      <td>0.079957</td>\n",
       "      <td>XGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fbs</td>\n",
       "      <td>0.085884</td>\n",
       "      <td>XGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>_full_model_</td>\n",
       "      <td>0.087284</td>\n",
       "      <td>XGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>trtbps</td>\n",
       "      <td>0.088470</td>\n",
       "      <td>XGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>thalachh</td>\n",
       "      <td>0.088470</td>\n",
       "      <td>XGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>restecg</td>\n",
       "      <td>0.092672</td>\n",
       "      <td>XGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>thall</td>\n",
       "      <td>0.094935</td>\n",
       "      <td>XGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>slp</td>\n",
       "      <td>0.100108</td>\n",
       "      <td>XGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>age</td>\n",
       "      <td>0.103987</td>\n",
       "      <td>XGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>sex</td>\n",
       "      <td>0.107435</td>\n",
       "      <td>XGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>cp</td>\n",
       "      <td>0.115194</td>\n",
       "      <td>XGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>caa</td>\n",
       "      <td>0.116703</td>\n",
       "      <td>XGBClassifier</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>_baseline_</td>\n",
       "      <td>0.508190</td>\n",
       "      <td>XGBClassifier</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        variable  dropout_loss          label\n",
       "0           chol      0.076509  XGBClassifier\n",
       "1           exng      0.079095  XGBClassifier\n",
       "2        oldpeak      0.079957  XGBClassifier\n",
       "3            fbs      0.085884  XGBClassifier\n",
       "4   _full_model_      0.087284  XGBClassifier\n",
       "5         trtbps      0.088470  XGBClassifier\n",
       "6       thalachh      0.088470  XGBClassifier\n",
       "7        restecg      0.092672  XGBClassifier\n",
       "8          thall      0.094935  XGBClassifier\n",
       "9            slp      0.100108  XGBClassifier\n",
       "10           age      0.103987  XGBClassifier\n",
       "11           sex      0.107435  XGBClassifier\n",
       "12            cp      0.115194  XGBClassifier\n",
       "13           caa      0.116703  XGBClassifier\n",
       "14    _baseline_      0.508190  XGBClassifier"
      ]
     },
     "execution_count": 273,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pvi.result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results interpretation\n",
    "\n",
    "The interpretation of the results of the method is straightforward. The values correspond to the decrease in the model score when the feature is randomly shuffled. The higher the value of the feature importance, the more important the feature is for the model. \n",
    "\n",
    "* The most important feature of the model is `caa` - the number of major vessels (0-4). After shuffling the values of this feature, the model loss is increased by 0.032. This is the highest value of the feature importance.\n",
    "* Another important feature is `cp` - chest pain type. After shuffling the values of this feature, the model loss is increased by 0.03. This is the second highest value of the feature importance.\n",
    "* The third most important feature is `sex`. After shuffling the values of this feature, the model loss is increased by 0.021.\n",
    "\n",
    "In this particular result, we can see that permutation of some features led to an `increase in model performance`. This is true for features:\n",
    "\n",
    "* `chol` - cholestoral in mg/dl\n",
    "* `exng` - exercise induced angina (1 = yes; 0 = no)\n",
    "* `oldpeak` - ST depression induced by exercise relative to rest\n",
    "\n",
    "It suggests, that the model is not very sensitive to these features. It is also possible that the model is overfitting to these features.\n",
    "\n",
    "Overall most of the features are important for the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pvi.plot(show=False).update_layout(autosize=False, width=600, height=450)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"vip1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need to have in mind while interpreting the results, is that the PVI method is stochastic. It means that for different seeds, we may see different results. To quantify the variability of the results, we can calculate some basic statistics of the feature importance.\n",
    "\n",
    "# Experiment - calculate results for different seeds\n",
    "\n",
    "I will now calculate the results for different seeds. I will calculate the mean and standard deviation of the feature importance and other interesting statistics. I will use the same settings as in the previous experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "metadata": {},
   "outputs": [],
   "source": [
    "def experiment(explainer, n_samples = 100):\n",
    "    pvi = explainer.model_parts(random_state=0)\n",
    "    df = pvi.result.transpose()\n",
    "    headers = df.iloc[0]\n",
    "    new_df  = pd.DataFrame(df.values[1:2], columns=headers)\n",
    "    for seed in range(1, n_samples):\n",
    "        pvi = explainer.model_parts(random_state=seed)\n",
    "        df = pvi.result.transpose()\n",
    "        headers = df.iloc[0]\n",
    "        df = pd.DataFrame(df.values[1:2], columns=headers)\n",
    "        new_df = new_df.append(df)\n",
    "    \n",
    "    new_df = new_df.astype(float)\n",
    "    new_df = new_df.drop(['_baseline_'], axis=1)\n",
    "    new_df.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    means_basic = new_df.mean().sort_values(ascending=True)\n",
    "    \n",
    "    return means_basic, new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_no_scaling, new_df_no_scaling = experiment(explainer, n_samples = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_boxplot = px.box(new_df_no_scaling, y=means_no_scaling.index , title='Boxplot of PVI for XGBoost model')\n",
    "fig_means = px.bar(means_no_scaling, title='Mean PVI for XGBoost model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_boxplot.show()\n",
    "fig_means.update_layout(showlegend=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig2.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig3.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment results interpretation\n",
    "\n",
    "After running the method 100 times, we can see that the results are quite stable. The mean and standard deviation of the feature importance is quite low, except for features: `thall` and `caa` where the standard deviation is quite high. \n",
    "\n",
    "On average, the most important feature is `cp` - chest pain type. \n",
    "The second most important feature is `caa` - the number of major vessels (0-4).\n",
    "\n",
    "Only two features fall below the full model performance: `chol` and `fbs`. This suggests that the model is not very sensitive to these features. It is also possible that the model is overfitting these features.\n",
    "\n",
    "## Comparison with previous results\n",
    "\n",
    "The results are quite similar to the previous results. The top 6 features are the same. This time, however, the order of the features is different. \n",
    "\n",
    "When it comes to the features that fall below the full model performance, the feature `chol` is still the least important feature. The feature `fbs` is now the second least important feature, but in the previous experiment, it was just above the full model performance. \n",
    "\n",
    "If we were to select features based on the PVI method, we would select all the features except for `chol` and `fbs`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison across models\n",
    "\n",
    "Now I will compare the results of the PVI method across many models. I will consider the following models:\n",
    "\n",
    "* xgboost:\n",
    "  * previous settings\n",
    "  * data scaled\n",
    "\n",
    "* logistic regression:\n",
    "  * default settings\n",
    "\n",
    "* random forest:\n",
    "  * default settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Xgboost with data scaling\n",
    "scaler = MinMaxScaler()\n",
    "data[['age', 'trtbps', 'chol', 'thalachh', 'oldpeak']] = scaler.fit_transform(data[['age', 'trtbps', 'chol', 'thalachh', 'oldpeak']])\n",
    "y = data['output'] # Target variable\n",
    "X = data.drop(['output'], axis=1) # Features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Xgboost\n",
    "model_scaled = xgboost.XGBClassifier(\n",
    "    n_estimators=200, \n",
    "    max_depth=4, \n",
    ")\n",
    "\n",
    "model_scaled.fit(X_train, y_train)\n",
    "\n",
    "explainer_scaled = dx.Explainer(model_scaled, X_test, y_test, predict_function=pf_xgboost_classifier_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression - default settings\n",
    "\n",
    "log_reg = LogisticRegression() \n",
    "log_reg.fit(X_train, y_train) \n",
    "\n",
    "explainer_log = dx.Explainer(log_reg, X_test, y_test, predict_function=pf_xgboost_classifier_categorical)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random forest - default settings\n",
    "forest_model = sklearn.ensemble.RandomForestClassifier()\n",
    "forest_model.fit(X_train, y_train)\n",
    "explainer_forest = dx.Explainer(forest_model, X_test, y_test, predict_function=pf_xgboost_classifier_categorical)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_basic, df_basic = experiment(explainer)\n",
    "means_log, df_log = experiment(explainer_log)\n",
    "means_forest, df_forest = experiment(explainer_forest)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reults interpretation\n",
    "\n",
    "## Top 2 features\n",
    "\n",
    "As we can see from the plot below, the results are quite different across models. The top-2 features are the same for logistic regression model and xgboost with data scaling. For the random forest model, the top-2 features are `cp` and `thalachh`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_basic = means_basic.sort_values(ascending=False)\n",
    "means_log = means_log.sort_values(ascending=False)\n",
    "means_forest = means_forest.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_df = pd.DataFrame({'xgboost': means_basic[:2], 'logistic regression': means_log[:2], 'random forest': means_forest[:2]})\n",
    "means_plot = px.bar(means_df, title=\"Feature importance barplot - top features\", barmode='group')\n",
    "means_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig4.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bottom 2 features\n",
    "\n",
    "For the XGBoost model, the bottom-2 features are `chol` and `fbs`. Same for the logistic regression model. The least important features for the random forest model are `chol` and `restecg`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "means_df = pd.DataFrame({'xgboost': means_basic[11:], 'logistic regression': means_log[11:], 'random forest': means_forest[11:]})\n",
    "means_plot = px.bar(means_df, title=\"Feature importance barplot - bottom features\", barmode='group')\n",
    "means_plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig5.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison of different methods for calculating feature importance - XGBoost\n",
    "\n",
    "Now I will compare the results of the PVI method with the results of the `feature importance` method for the xgboost model. The feature importance in the built-in method for xgboost is calculated for a single decision tree by the amount that each attribute split point improves the performance measure, weighted by the number of observations the node is responsible for. The performance measure is the purity (Gini index) used to select the split points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [],
   "source": [
    "built_in = model.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_compare = pd.DataFrame({'built-in method': built_in, 'PVI': means_no_scaling.drop('_full_model_')})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_comparison = px.bar(df_compare, title='Comparison of built-in method and PVI', barmode='group')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results interpretation\n",
    "\n",
    "The built-in method based on Gini impurity is quite different from the PVI method. The Gini impurity shows much more variability across features - the standard deviation is much higher. The PVI method shows much more stable results. However, it can be due to the fact that the PVI method was calculated 100 times for each feature, while the built-in method was calculated only once.\n",
    "Nevertheless, the top-1 feature is the same - `cp` - chest pain type and the bottom-1 feature is the same - `chol` - cholesterol in mg/dl."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig6.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.get_dummies(data, columns=['sex', 'cp', 'fbs', 'restecg', 'exng', 'slp', 'caa', 'thall'])\n",
    "\n",
    "y = data['output'] # Target variable\n",
    "X = data.drop(['output'], axis=1) # Features\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Xgboost\n",
    "model = xgboost.XGBClassifier(\n",
    "    n_estimators=200, \n",
    "    max_depth=4, \n",
    ")\n",
    "\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explainer_shap = dx.Explainer(model, X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_vi = explainer_shap.model_parts(type=\"shap_wrapper\", shap_explainer_type=\"TreeExplainer\", check_additivity=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SHAP variable importance based on tree method\n",
    "\n",
    "Now we will compare the results of the PVI method with the results of the SHAP variable importance based on tree method. The SHAP tree method is based on the Shapley value.\n",
    "\n",
    "## Results interpretation\n",
    "\n",
    "The results are harder to interpret because of one-hot encoding. \n",
    "\n",
    "For the SHAP tree method, the most important factor influencing model performance is `caa_0` - the number of major vessels = 0. The variable `oldpeak` which has not appeared in the top-2 features for the PVI method across different models is now the second most important feature. Another thing worth noting is that the feature `chol`, which for many models was the least important feature, is now the third most important feature. What we need to keep in mind, however, is that for variables `oldpeak` and `chol` the values are not consistent with the impact on the model performance. We see high values of `chol` impacting positively on the model performance and negatively (violet dots on both sides of the center line). Same observation for `oldpeak`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_vi.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"fig7.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "In this notebook, I have shown how to calculate the permutation-based variable importance for the xgboost model. I have also compared the results of the PVI method with the results of the `feature importance` method for the xgboost model and the results of the SHAP variable importance based on tree method."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('xai')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "92d0492df59c13792d4005f40ce57722ea1004eeb0f1cc5d82cf0cbcd49d58f9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
