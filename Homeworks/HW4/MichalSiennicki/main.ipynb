{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Task 1\n",
    "\n",
    "$$\n",
    "g_{PD}^1(z) =\n",
    "E_{X^{-j}}f(X^{j|=z}) =\n",
    "E_{x_2} (z^2 + 2zx_2 + x_2^2) =\n",
    "z^2 + 2zE_{x_2} x_2 + E_{x_2} x_2^2 =\n",
    "\\\\ =\n",
    "z^2 + 2z \\cdot 0 + E_{x_2} x_2^2 =\n",
    "z^2 + E_{x_2 \\in [0, 1]} x_2^2 =\n",
    "z^2 + \\frac{1}{3}\n",
    "$$\n",
    "\n",
    "$$\n",
    "g_{MP}^1(z) =\n",
    "E_{X^{-j}|x^j=z}f(x^{j|=z}) =\n",
    "E_{x_2|x_1=z} (z+x_2)^2 =\n",
    "4z^2\n",
    "$$\n",
    "\n",
    "$$\n",
    "g_{AL}^1(z) =\n",
    "\\int_{z_0}^z [E_{X^{-j}|X^j=v} \\frac{\\partial f(x)}{\\partial x_j}]dv =\n",
    "\\int_{-1}^z E_{x_2|x_1=v} \\frac{\\partial(x_1^2 + 2x_1x_2 + x_2^2)}{\\partial x_1} dv + c =\n",
    "\\\\ =\n",
    "\\int_{-1}^z E_{x_2|x_1=v} (2x_1 + 2x_2) dv + c =\n",
    "\\int_{-1}^z (2v + 2v)dv + c =\n",
    "4 \\int_{-1}^z v dv + c =\n",
    "2z^2 - 2 + c\n",
    "$$"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Task 2"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.\n",
    "\n",
    "Here are two Ceteris Paribus profiles (my own implementation). I'm sampling 50 different (equally distributed between max and min) possible values of age and calculating the model's prediction. As we can see, the model changes its prediction a lot, the plot is not smooth.\n",
    "\n",
    "| CP - Obs. 1.      | CP - Obs. 3.      |\n",
    "|-------------------|-------------------|\n",
    "| ![](imgs/2/0.png) | ![](imgs/2/2.png) |\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3.\n",
    "\n",
    "As above, here are two Ceteris Paribus profiles (my own implementation). Those two differ much:\n",
    "- the first has lower prediction score for age in interval [55, 65]\n",
    "- the second has lower prediction score for age bigger than 60 and higher score in the interval [50, 60]\n",
    "\n",
    "This is quite common with Random Forest model, it looks at a lot of different correlations - different observations will behave differently when changing one variable\n",
    "\n",
    "| CP - Obs. 1.      | CP - Obs. 2.      |\n",
    "|-------------------|-------------------|\n",
    "| ![](imgs/3/0.png) | ![](imgs/3/1.png) |\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4.\n",
    "\n",
    "Here there is a PDP plot (my own implementation). I calculate the mean over whole test dataset Ceteris Paribus profiles.\n",
    "In the background we can see all singular CP plots.\n",
    "This gives much better understanding of the whole model. There is a high variance in CP plots, taking the mean of them reduces it.\n",
    "As we can see, the model in general tends to give smaller score if the age is close to 60.\n",
    "\n",
    "| PDP                 | CP - Obs. 1.      |\n",
    "|---------------------|-------------------|\n",
    "| ![](imgs/4/pdp.png) | ![](imgs/4/0.png) |\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5.\n",
    "\n",
    "Here are two PDP plots - for the linear regression and for the random forest model.\n",
    "The difference is quite clear:\n",
    "- Linear regression can have only linear CP plots, so the PDP (mean) is also linear. This model gives lower prediction to young patients.\n",
    "- Random Forest has more complicated CP plots. On the PDP plot we can see that this model gives lower score for ages close to 60 and a bit higher for ages close to 55.\n",
    "\n",
    "\n",
    "| PDP - Linear Model                 | PDP - Random Forest                    |\n",
    "|------------------------------------|----------------------------------------|\n",
    "| ![](imgs/5/LogisticRegression.png) | ![](imgs/5/RandomForestClassifier.png) |\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Appendix\n",
    "\n",
    "## 0.\n",
    "Here the data is loaded (same as in previous homework) and a simple model is trained and evaluated. It is Random Forest Classifier from sklearn with default parameters.\n",
    "\n",
    "Loading and preparing the data consists of:\n",
    "- one hot encoding (models like logistic regression require this)\n",
    "- splitting between target (y) and x"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape=(242, 22) x_test.shape=(61, 22) y_train.shape=(242,) y_test.shape=(61,)\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "warnings.simplefilter(action='ignore', category=UserWarning)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "\n",
    "def load_data():\n",
    "    df = pd.read_csv('heart.csv')\n",
    "\n",
    "    # One hot encoding (for linear classifier)\n",
    "    df = pd.get_dummies(df, columns=['caa', 'cp', 'restecg'])\n",
    "\n",
    "    # Get targets\n",
    "    y_all = df['output'].to_numpy()\n",
    "\n",
    "    x_all = df.drop(columns=['output'])\n",
    "\n",
    "    # Split data to train and test\n",
    "    return train_test_split(x_all, y_all, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "x_train, x_test, y_train, y_test = load_data()\n",
    "print(f\"{x_train.shape=}\", f\"{x_test.shape=}\", f\"{y_train.shape=}\", f\"{y_test.shape=}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Training the models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'auc': 0.8841594827586207, 'accuracy': 0.8852459016393442}\n",
      "{'auc': 0.8685344827586206, 'accuracy': 0.8688524590163934}\n"
     ]
    }
   ],
   "source": [
    "def get_model(verbose=False, model=None):\n",
    "    if model is None:\n",
    "        model = RandomForestClassifier()\n",
    "    metrics = {\n",
    "        \"auc\": roc_auc_score,\n",
    "        \"accuracy\": accuracy_score\n",
    "    }\n",
    "    model.fit(x_train, y_train)\n",
    "    pred_test = model.predict(x_test)\n",
    "    if verbose:\n",
    "        print({metric_name: metric_fun(y_test, pred_test) for metric_name, metric_fun in metrics.items()})\n",
    "\n",
    "    return model\n",
    "\n",
    "model = get_model(True)\n",
    "model_linear = get_model(True, LogisticRegression())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1.\n",
    "Here are the model predictions on the first two observations from the test set"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.06 0.61]\n"
     ]
    }
   ],
   "source": [
    "print(model.predict_proba(x_test.iloc[0:2])[:, 1])"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "col = 'age'\n",
    "\n",
    "def get_plot_data(n = 50, m=model):\n",
    "    minmax = x_test[col].min(), x_test[col].max()\n",
    "\n",
    "    plot = pd.DataFrame()\n",
    "    for i in range(n):\n",
    "        data = x_test.copy()\n",
    "        val = (minmax[0] * i + minmax[1] * (n-i)) / n\n",
    "        data[col] = val\n",
    "        preds = m.predict_proba(data)[:, 1]\n",
    "        plot = plot.append({\n",
    "            col: val,\n",
    "            **{f\"prediction (obs. {j+1}.)\": preds[j] for j in range(len(preds))},\n",
    "            \"prediction mean\": preds.mean(axis=0),\n",
    "        }, ignore_index=True)\n",
    "    return plot\n",
    "\n",
    "\n",
    "plot = get_plot_data()\n",
    "for j in [0, 2]:\n",
    "    px.line(plot, x=col, y=plot.columns[1+j]).write_image(f\"imgs/2/{j}.png\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 3."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "for j in [0, 1]:\n",
    "    px.line(plot, x=col, y=plot.columns[1+j]).write_image(f\"imgs/3/{j}.png\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 4."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "for j in [0, 1, 2]:\n",
    "    px.line(plot, x=col, y=plot.columns[1+j]).write_image(f\"imgs/4/{j}.png\")\n",
    "\n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x=plot[col], y=plot[\"prediction mean\"], mode='lines', name=\"prediction mean\"))\n",
    "for c in plot.columns[1:-1]:\n",
    "    fig.add_trace(go.Scatter(x=plot[col], y=plot[c], mode='lines', name='lines', showlegend=False, opacity=0.1))\n",
    "\n",
    "fig.write_image(f\"imgs/4/pdp.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 5."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "for m in [model, model_linear]:\n",
    "    plot = get_plot_data(m=m)\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(go.Scatter(x=plot[col], y=plot[\"prediction mean\"], mode='lines', name=\"prediction mean\"))\n",
    "    for c in plot.columns[1:-1]:\n",
    "        fig.add_trace(go.Scatter(x=plot[col], y=plot[c], mode='lines', name='lines', showlegend=False, opacity=0.1))\n",
    "\n",
    "    fig.write_image(f\"imgs/5/{m.__class__.__name__}.png\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}